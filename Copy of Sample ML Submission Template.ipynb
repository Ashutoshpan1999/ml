{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"10B2s5af8INkHgIz_7XBjW3iNDbGY7ltx","timestamp":1721058120104}],"collapsed_sections":["vncDsAP0Gaoa","FJNUwmbgGyua","w6K7xa23Elo4","yQaldy8SH6Dl","mDgbUHAGgjLW","O_i_v8NEhb9l","HhfV-JJviCcP","Y3lxredqlCYt","3RnN4peoiCZX","x71ZqKXriCWQ","7hBIi_osiCS2","JlHwYmJAmNHm","35m5QtbWiB9F","PoPl-ycgm1ru","H0kj-8xxnORC","nA9Y7ga8ng1Z","PBTbrJXOngz2","u3PMJOP6ngxN","dauF4eBmngu3","bKJF3rekwFvQ","MSa1f5Uengrz","GF8Ens_Soomf","0wOQAZs5pc--","K5QZ13OEpz2H","lQ7QKXXCp7Bj","448CDAPjqfQr","KSlN3yHqYklG","t6dVpIINYklI","ijmpgYnKYklI","-JiQyfWJYklI","EM7whBJCYoAo","fge-S5ZAYoAp","85gYPyotYoAp","RoGjAbkUYoAp","4Of9eVA-YrdM","iky9q4vBYrdO","F6T5p64dYrdO","y-Ehk30pYrdP","bamQiAODYuh1","QHF8YVU7Yuh3","GwzvFGzlYuh3","qYpmQ266Yuh3","OH-pJp9IphqM","bbFf2-_FphqN","_ouA3fa0phqN","Seke61FWphqN","PIIx-8_IphqN","t27r6nlMphqO","r2jJGEOYphqO","b0JNsNcRphqO","BZR9WyysphqO","jj7wYXLtphqO","eZrbJ2SmphqO","rFu4xreNphqO","YJ55k-q6phqO","gCFgpxoyphqP","OVtJsKN_phqQ","lssrdh5qphqQ","U2RJ9gkRphqQ","1M8mcRywphqQ","tgIPom80phqQ","JMzcOPDDphqR","x-EpHcCOp1ci","X_VqEhTip1ck","8zGJKyg5p1ck","PVzmfK_Ep1ck","n3dbpmDWp1ck","ylSl6qgtp1ck","ZWILFDl5p1ck","M7G43BXep1ck","Ag9LCva-p1cl","E6MkPsBcp1cl","2cELzS2fp1cl","3MPXvC8up1cl","NC_X3p0fY2L0","UV0SzAkaZNRQ","YPEH6qLeZNRQ","q29F0dvdveiT","EXh0U9oCveiU","22aHeOlLveiV","g-ATYxFrGrvw","Yfr_Vlr8HBkt","8yEUt7NnHlrM","tEA2Xm5dHt1r","I79__PHVH19G","Ou-I18pAyIpj","fF3858GYyt-u","4_0_7-oCpUZd","hwyV_J3ipUZe","3yB-zSqbpUZe","dEUvejAfpUZe","Fd15vwWVpUZf","bn_IUdTipZyH","49K5P_iCpZyH","Nff-vKELpZyI","kLW572S8pZyI","dWbDXHzopZyI","yLjJCtPM0KBk","xiyOF9F70UgQ","7wuGOrhz0itI","id1riN9m0vUs","578E2V7j08f6","89xtkJwZ18nB","67NQN5KX2AMe","Iwf50b-R2tYG","GMQiZwjn3iu7","WVIkgGqN3qsr","XkPnILGE3zoT","Hlsf0x5436Go","mT9DMSJo4nBL","c49ITxTc407N","OeJFEK0N496M","9ExmJH0g5HBk","cJNqERVU536h","k5UmGsbsOxih","T0VqWOYE6DLQ","qBMux9mC6MCf","-oLEiFgy-5Pf","C74aWNz2AliB","2DejudWSA-a0","pEMng2IbBLp7","rAdphbQ9Bhjc","TNVZ9zx19K6k","nqoHp30x9hH9","rMDnDkt2B6du","yiiVWRdJDDil","1UUpS68QDMuG","kexQrXU-DjzY","T5CmagL3EC8N","BhH2vgX9EjGr","qjKvONjwE8ra","P1XJ9OREExlT","VFOzZv6IFROw","TIqpNgepFxVj","VfCC591jGiD4","OB4l2ZhMeS1U","ArJBuiUVfxKd","4qY1EAkEfxKe","PiV4Ypx8fxKe","TfvqoZmBfxKf","dJ2tPlVmpsJ0","JWYfwnehpsJ1","-jK_YjpMpsJ2","HAih1iBOpsJ2","zVGeBEFhpsJ2","bmKjuQ-FpsJ3","Fze-IPXLpx6K","7AN1z2sKpx6M","9PIHJqyupx6M","_-qAgymDpx6N","Z-hykwinpx6N","h_CCil-SKHpo","cBFFvTBNJzUa","HvGl1hHyA_VK","EyNgTHvd2WFk","KH5McJBi2d8v","iW_Lq9qf2h6X","-Kee-DAl2viO","gCX9965dhzqZ","gIfDvo9L0UH2"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Project Name**    -\n","\n"],"metadata":{"id":"vncDsAP0Gaoa"}},{"cell_type":"markdown","source":["##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n","##### **Contribution**    - Individual/Team\n","##### **Team Member 1 -**\n","##### **Team Member 2 -**\n","##### **Team Member 3 -**\n","##### **Team Member 4 -**"],"metadata":{"id":"beRrZCGUAJYm"}},{"cell_type":"markdown","source":["# **Project Summary -**"],"metadata":{"id":"FJNUwmbgGyua"}},{"cell_type":"markdown","source":["Write the summary here within 500-600 words."],"metadata":{"id":"F6v_1wHtG2nS"}},{"cell_type":"markdown","source":["# **GitHub Link -**"],"metadata":{"id":"w6K7xa23Elo4"}},{"cell_type":"markdown","source":["Provide your GitHub Link here."],"metadata":{"id":"h1o69JH3Eqqn"}},{"cell_type":"markdown","source":["# **Problem Statement**\n"],"metadata":{"id":"yQaldy8SH6Dl"}},{"cell_type":"markdown","source":["**Write Problem Statement Here.**"],"metadata":{"id":"DpeJGUA3kjGy"}},{"cell_type":"markdown","source":["# **General Guidelines** : -  "],"metadata":{"id":"mDgbUHAGgjLW"}},{"cell_type":"markdown","source":["1.   Well-structured, formatted, and commented code is required.\n","2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n","     \n","     The additional credits will have advantages over other students during Star Student selection.\n","       \n","             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n","                       without a single error logged. ]\n","\n","3.   Each and every logic should have proper comments.\n","4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n","        \n","\n","```\n","# Chart visualization code\n","```\n","            \n","\n","*   Why did you pick the specific chart?\n","*   What is/are the insight(s) found from the chart?\n","* Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","5. You have to create at least 15 logical & meaningful charts having important insights.\n","\n","\n","[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n","\n","U - Univariate Analysis,\n","\n","B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n","\n","M - Multivariate Analysis\n"," ]\n","\n","\n","\n","\n","\n","6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n","\n","\n","*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n","\n","\n","*   Cross- Validation & Hyperparameter Tuning\n","\n","*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n","\n","*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ZrxVaUj-hHfC"}},{"cell_type":"markdown","source":["# ***Let's Begin !***"],"metadata":{"id":"O_i_v8NEhb9l"}},{"cell_type":"markdown","source":["## ***1. Know Your Data***"],"metadata":{"id":"HhfV-JJviCcP"}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"Y3lxredqlCYt"}},{"cell_type":"code","source":["# Import Libraries\n","! pip install scikit-optimize"],"metadata":{"id":"M8Vqi-pPk-HR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import Libraries\n","\n","# Data visualization libraries(matplotlib,seaborn, plotly)\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","\n","# Datetime library for manipulating Date columns.\n","from datetime import datetime\n","import datetime as dt\n","\n","\n","# from sci-kit library scaling, transforming and labeling functions are brought\n","# which is used to change raw feature vectors into a representation that is more\n","# suitable for the downstream estimators.\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","\n","# Importing various machine learning models.\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Lasso\n","from sklearn.linear_model import Ridge\n","from sklearn.linear_model import ElasticNet\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.ensemble import GradientBoostingRegressor\n","\n","from sklearn.model_selection import cross_validate\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.model_selection import RandomizedSearchCV\n","\n","# Import different metrics from sci-kit libraries for model evaluation.\n","from sklearn import metrics\n","from sklearn.metrics import r2_score\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import log_loss\n","\n","# Necessary imports\n","\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from scipy.stats import yeojohnson\n","from sklearn.ensemble import GradientBoostingRegressor\n","from skopt import BayesSearchCV\n","from skopt.space import Real, Integer\n","\n","# Importing warnings library. The warnings module handles warnings in Python.\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"fPMxx9S0GGiO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"3RnN4peoiCZX"}},{"cell_type":"code","source":["# Load Dataset\n","from google.colab import drive                #Mounting google drive\n","drive.mount('/content/drive')"],"metadata":{"id":"4CkvbW_SlZ_R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the Seoul bike dataset from Google Drive using pd.read_csv\n","# The 'encoding='latin'' parameter is used to specify the character encoding of the file, ensuring proper reading of non-English characters or special symbols\n","bike_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/SeoulBikeData.csv', encoding='latin')"],"metadata":{"id":"A9azsSb1HbCA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset First View"],"metadata":{"id":"x71ZqKXriCWQ"}},{"cell_type":"code","source":["# Dataset First Look\n","bike_df.head()"],"metadata":{"id":"LWNFOSvLl09H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Last 5 rows\n","bike_df.tail()"],"metadata":{"id":"ECZWUkiDIVJF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Rows & Columns count"],"metadata":{"id":"7hBIi_osiCS2"}},{"cell_type":"code","source":["# Dataset Rows & Columns count\n","# Dataset Rows & Columns count\n","num_rows, num_cols = bike_df.shape\n","\n","print(\"Number of rows:\", num_rows)\n","print(\"Number of columns:\", num_cols)"],"metadata":{"id":"Kllu7SJgmLij"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Information"],"metadata":{"id":"JlHwYmJAmNHm"}},{"cell_type":"code","source":["# Dataset Info\n","bike_df.info()"],"metadata":{"id":"e9hRXRi6meOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Duplicate Values"],"metadata":{"id":"35m5QtbWiB9F"}},{"cell_type":"code","source":["# Dataset Duplicate Value Count\n","bike_df.duplicated().sum()"],"metadata":{"id":"1sLdpKYkmox0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Missing Values/Null Values"],"metadata":{"id":"PoPl-ycgm1ru"}},{"cell_type":"code","source":["# Missing Values/Null Values Count\n","bike_df.isna().sum()"],"metadata":{"id":"GgHWkxvamxVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing the missing values\n","plt.figure(figsize=(20,8))\n","sns.heatmap(bike_df.isna().transpose(), cmap=\"viridis\", cbar_kws={'label': 'Missing Data'})\n","plt.title('Visualization of Missing Values', fontsize=18)\n","plt.show()"],"metadata":{"id":"3q5wnI3om9sJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What did you know about your dataset?"],"metadata":{"id":"H0kj-8xxnORC"}},{"cell_type":"markdown","source":["\n","There are 8760 observation and 14 features.\n","In a day we have 24 hours and we have 365 days a year so 365 multiplied by 24 = 8760, which represents the number of line in the dataset\n","There are no null values.\n","Dataset has all unique values i.e., there is no duplicate, which means data is free from bias as duplicates which can cause problems in downstream analysis, such as biasing results or making it difficult to accurately summarize the data.\n","Date has some object data types, it should be datetime data type.\n","```\n","# This is formatted as code\n","```\n","\n","Answer Here"],"metadata":{"id":"gfoNAAC-nUe_"}},{"cell_type":"markdown","source":["## ***2. Understanding Your Variables***"],"metadata":{"id":"nA9Y7ga8ng1Z"}},{"cell_type":"code","source":["# Dataset Columns\n","bike_df.columns"],"metadata":{"id":"j7xfkqrt5Ag5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset Describe\n","bike_df.describe(include='all').round(2)"],"metadata":{"id":"DnOaZdaE5Q5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Variables Description"],"metadata":{"id":"PBTbrJXOngz2"}},{"cell_type":"markdown","source":["Features Breakdown:-\n","Date: The day's date, ranging from 01/12/2017 to 30/11/2018, formatted as DD/MM/YYYY (string). Conversion to datetime format required.\n","\n","Rented Bike Count: Number of rented bikes per hour, our dependent variable for prediction (integer).\n","\n","Hour: The hour of the day, ranging from 0 to 23 in digital time format (integer). Conversion to categorical data type needed.\n","\n","Temperature(°C): Temperature in Celsius (float).\n","\n","Humidity(%): Air humidity percentage (integer).\n","\n","Wind speed (m/s): Wind speed in meters per second (float).\n","\n","Visibility (10m): Visibility in meters (integer).\n","\n","Dew point temperature(°C): Morning temperature (float).\n","\n","Solar Radiation (MJ/m2): Sun contribution (float).\n","\n","Rainfall(mm): Amount of rainfall in millimeters (float).\n","\n","Snowfall (cm): Amount of snowfall in centimeters (float).\n","\n","Seasons: Season of the year (string), limited to four seasons.\n","\n","Holiday: Indicates if the day is a holiday period (string).\n","\n","Functioning Day: Indicates if the day is a functioning day (string).Answer Here"],"metadata":{"id":"aJV4KIxSnxay"}},{"cell_type":"markdown","source":["### Check Unique Values for each variable."],"metadata":{"id":"u3PMJOP6ngxN"}},{"cell_type":"code","source":["# Check Unique Values for each variable.\n","for i in bike_df.columns.tolist():\n","  print(f\"No. of unique values in {i} is {bike_df[i].nunique()}.\")"],"metadata":{"id":"zms12Yq5n-jE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. ***Data Wrangling***"],"metadata":{"id":"dauF4eBmngu3"}},{"cell_type":"markdown","source":["### Data Wrangling Code"],"metadata":{"id":"bKJF3rekwFvQ"}},{"cell_type":"code","source":["# Write your code to make your dataset analysis ready.\n","bike_df_1 = bike_df.copy()"],"metadata":{"id":"wk-9a2fpoLcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Some of  the columns name in the dataset are too large and clumsy so we change them into some simple name, and it don't affect our end results.\n","# Renaming the Columns\n","\n","bike_df_1.rename(columns= {'Rented Bike Count':'Rented_Bike_Count',\n","                                'Temperature(°C)':'Temperature',\n","                                'Humidity(%)':'Humidity',\n","                                'Wind speed (m/s)':'Wind_speed',\n","                                'Visibility (10m)':'Visibility',\n","                                'Dew point temperature(°C)':'Dew_point_temperature',\n","                                'Solar Radiation (MJ/m2)':'Solar_Radiation',\n","                                'Rainfall(mm)':'Rainfall',\n","                                'Snowfall (cm)':'Snowfall',\n","                                'Functioning Day':'Functioning_Day'}, inplace=True)"],"metadata":{"id":"mEPYWCDXLgQs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bike_df_1.columns"],"metadata":{"id":"X0jcJ7fLLpLj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"MSa1f5Uengrz"}},{"cell_type":"markdown","source":["In Python, the \"Date\" column is read as an object type, essentially as a string. Since the date column is crucial for analyzing user behavior, it needs to be converted into a datetime format. After this conversion, we will split it into three columns: 'year', 'month', and 'day', each as a category data type.Answer Here."],"metadata":{"id":"LbyXE7I1olp8"}},{"cell_type":"code","source":["# converting date variable into datetime format\n","bike_df_1['Date'] = bike_df_1['Date'].apply(lambda x: dt.datetime.strptime(x, '%d/%m/%Y'))"],"metadata":{"id":"39sHJRWYMZCJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the \"Date\" column into three \"year\",\"month\",\"day\" columns\n","bike_df_1['year'] = bike_df_1['Date'].dt.year\n","bike_df_1['month'] = bike_df_1['Date'].dt.month\n","bike_df_1['day'] = bike_df_1['Date'].dt.day_name()"],"metadata":{"id":"gS0FxmIvNW46"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We splited the \"date\" column into 3 different columns: \"year\", \"month\", \"day\".\n","The \"year\" column in our dataset contains 2 unique numbers detailing from December 2017 to November 2018. Considering this as a single year, we can drop the \"year\" column.\n","The \"day\" column contains details about each day of the month. For our purposes, we only need to know if a day is a weekday or a weekend, so we convert it into this format and drop the \"day\" column."],"metadata":{"id":"bUx3jKxpNf7X"}},{"cell_type":"code","source":["# Creating a new column of \"weekdays_weekend\" and drop the column \"Date\",\"day\",\"year\"\n","bike_df_1['weekdays_weekend']=bike_df_1['day'].apply(lambda x : 1 if x=='Saturday' or x=='Sunday' else 0 )    # 0 for weekdays and 1 for weekends\n","bike_df_1=bike_df_1.drop(columns=['Date','day','year'], axis=1)"],"metadata":{"id":"x2PixynPNnTQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bike_df_1.head()"],"metadata":{"id":"GEd6BwHZNxl9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bike_df_1['weekdays_weekend'].value_counts()"],"metadata":{"id":"E8rqpSndN5TX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As the \"Hour,\" \"month,\" and \"weekdays_weekend\" columns are currently shown as integer data types, they should actually be categorized as category data types. Failing to do so may lead to inaccurate analysis and correlations, potentially resulting in misleading conclusions."],"metadata":{"id":"1ZA4gTEVOAAj"}},{"cell_type":"code","source":["# Change the int64 columns into category columns\n","cols=['Hour','month','weekdays_weekend']\n","for col in cols:\n","  bike_df_1[col]=bike_df_1[col].astype('category')"],"metadata":{"id":"kqizYcCmOlns"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the dtypes again\n","bike_df_1.info()"],"metadata":{"id":"qCuy2kTLOpqZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# defining continuous independent variables separately\n","cont_var = ['Temperature', 'Humidity', 'Wind_speed', 'Visibility', 'Dew_point_temperature','Solar_Radiation', 'Rainfall', 'Snowfall']\n",""],"metadata":{"id":"S7CSMfAMO52h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# defining dependent variable\n","dependent_variable = ['Rented_Bike_Count']\n",""],"metadata":{"id":"Qy5RldFWO9vG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# defining categorical independent variables separately\n","cat_var = ['Hour','Seasons', 'Holiday', 'Functioning_Day', 'month', 'weekdays_weekend']\n",""],"metadata":{"id":"poFs4vhxPDo9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What all manipulations have you done and insights you found?\n","\n","Some of the columns' names in the dataset were excessively long and cumbersome, so we simplified them. This modification did not impact our final results.\n","The \"Date\" column in the dataset was initially read as an object type in Python, essentially as a string. Recognizing the significance of the date column for analyzing user behavior, we converted it into a datetime format.\n","Following this conversion, we split it into three columns: 'year', 'month', and 'day', each as a category data type.\n","The \"year\" column in our dataset contains 2 unique numbers detailing from December 2017 to November 2018. Treating this as a single year, we dropped the \"year\" column.\n","The \"day\" column contains details about each day of the month. For our purposes, we only needed to know if a day is a weekday or a weekend, so we converted it into this format and dropped the \"day\" column.\n","The \"Hour,\" \"month,\" and \"weekdays_weekend\" columns were initially shown as integer data types. We categorized them as category data types to ensure accurate analysis and correlations, thereby avoiding potentially misleading conclusions."],"metadata":{"id":"aA597BQMPLgL"}},{"cell_type":"markdown","source":["## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"],"metadata":{"id":"GF8Ens_Soomf"}},{"cell_type":"markdown","source":["#### Chart - 1"],"metadata":{"id":"0wOQAZs5pc--"}},{"cell_type":"code","source":["\n","# Chart-1 Visualization code for distribution of target variable\n","plt.figure(figsize=(8,5))\n","sns.distplot(bike_df_1['Rented_Bike_Count'], color='blue')\n","plt.show()\n",""],"metadata":{"id":"7v_ESjsspbW7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"K5QZ13OEpz2H"}},{"cell_type":"markdown","source":["A distplot, also referred to as a histogram with a kernel density estimate (KDE) plot, is valuable as it offers a swift and straightforward method to examine data distribution, detect patterns or outliers, and compare the distribution of multiple variables. It also facilitates the assessment of whether the data adheres to a normal distribution.\n","\n","Consequently, I utilized the histogram plot to analyze the distribution of variables across the entire dataset, determining symmetry.Answer Here."],"metadata":{"id":"XESiWehPqBRc"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"lQ7QKXXCp7Bj"}},{"cell_type":"markdown","source":["Based on the distribution plot of the dependent variable \"rented bike\", it is evident that the distribution is positively skewed (right skewed), indicating asymmetry around the mean.Answer Here"],"metadata":{"id":"C_j1G7yiqdRP"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"448CDAPjqfQr"}},{"cell_type":"markdown","source":["Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","Certainly, based on this insight, it's clear that our data is not normally distributed. Therefore, prior to implementing any model on this dataset, it's essential to normalize the data.Answer Here"],"metadata":{"id":"3cspy4FjqxJW"}},{"cell_type":"markdown","source":["#### Chart - 2"],"metadata":{"id":"KSlN3yHqYklG"}},{"cell_type":"code","source":["\n","# Visualizing code of histogram plot & boxplot for each columns to know the data distribution\n","for col in bike_df_1.describe().columns:\n","    fig,axes = plt.subplots(nrows=1,ncols=2,figsize=(15,4))\n","    sns.histplot(bike_df_1[col], ax = axes[0],kde = True, color='blue')\n","    sns.boxplot(bike_df_1[col], ax = axes[1],orient='h',showmeans=True,color='orange')\n","    fig.suptitle(\"Distribution plot of \"+ col, fontsize = 15)\n","    plt.show()"],"metadata":{"id":"R4YgtaqtYklH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t6dVpIINYklI"}},{"cell_type":"markdown","source":["A histplot is a chart that displays the distribution of a dataset, providing a graphical representation of how often each value or group of values occurs. It is valuable for understanding the dataset's distribution, identifying patterns or trends, and is particularly useful for large datasets (exceeding 100 observations) to detect outliers or gaps in the data.\n","\n","Consequently, we utilized the histogram plot to analyze the variable distributions across the entire dataset for symmetry.\n","\n","A boxplot summarizes key statistical characteristics of a dataset, including the median, quartiles, and range, in a single plot. It is useful for identifying outliers, comparing multiple datasets, and understanding data dispersion, commonly employed in statistical analysis and data visualization.\n","\n","Therefore, for each numerical variable in the given dataset, we used a box plot to analyze outliers and the interquartile range, encompassing mean, median, maximum, and minimum values.Answer Here."],"metadata":{"id":"5aaW0BYyYklI"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ijmpgYnKYklI"}},{"cell_type":"markdown","source":["Based on the above univariate analysis of all continuous feature variables, it is evident that only the temperature and humidity columns exhibit a normal distribution, while the others display different distributions.\n","\n","Furthermore, outlier values are noticeable in the snowfall, rainfall, wind speed, and solar radiation columns.Answer Here"],"metadata":{"id":"PSx9atu2YklI"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"-JiQyfWJYklI"}},{"cell_type":"markdown","source":["Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","Histogram and Box plot cannot give us whole information regarding data. It's done just to see the distribution of the column data over the dataset.Answer Here"],"metadata":{"id":"BcBbebzrYklV"}},{"cell_type":"markdown","source":["#### Chart - 3"],"metadata":{"id":"EM7whBJCYoAo"}},{"cell_type":"code","source":["# Analyzing the relationship between the dependent variable and the continuous variables\n","for i in cont_var:\n","  plt.figure(figsize=(11,5))\n","  sns.regplot(x=i,y=dependent_variable[0],data=bike_df_1)\n","  plt.xlabel(i)\n","  plt.ylabel(dependent_variable[0])\n","  plt.title(i+' vs '+ dependent_variable[0])\n","  plt.show()"],"metadata":{"id":"t6GMdE67YoAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"fge-S5ZAYoAp"}},{"cell_type":"markdown","source":["The regplot function is utilized to generate a scatter plot with a linear regression line. Its purpose is to visualize the relationship between two continuous variables, aiding in the identification of patterns and trends in the data. Additionally, it can be employed to test for linearity and independence of the variables.\n","\n","We utilized this regplot to examine the patterns between the independent variable and our dependent variable, \"rented bike.\"Answer Here."],"metadata":{"id":"5dBItgRVYoAp"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"85gYPyotYoAp"}},{"cell_type":"markdown","source":["From above regression plot we can see that there is some linearity between temperature, solar radiation & dew point temperature with dependent variable rented bike\n","\n","Other variables are not showing any patterns."],"metadata":{"id":"4jstXR6OYoAp"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"RoGjAbkUYoAp"}},{"cell_type":"markdown","source":["Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","Indeed, the regplot provided valuable insights, indicating that certain variables exhibit patterns with the dependent variable. These variables may be crucial features when predicting rented bike counts, warranting focused attention from the business.Answer Here"],"metadata":{"id":"zfJ8IqMcYoAp"}},{"cell_type":"markdown","source":["#### Chart - 4"],"metadata":{"id":"4Of9eVA-YrdM"}},{"cell_type":"code","source":["# Analyzing the relationship between the dependent variable and the categorical variables\n","for i in cat_var:\n","    plt.figure(figsize=(11, 4))\n","    sns.barplot(x=i, y=bike_df_1[dependent_variable[0]], data=bike_df_1, palette=\"muted\")\n","    plt.xlabel(i)\n","    plt.ylabel(dependent_variable[0])\n","    plt.title(i + ' vs ' + dependent_variable[0])\n","    plt.show()\n"],"metadata":{"id":"irlUoxc8YrdO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"iky9q4vBYrdO"}},{"cell_type":"markdown","source":["Bar charts are employed to compare the size or frequency of different categories or groups of data. They are valuable for comparing data across various categories and can effectively display a large amount of data in a small space.\n","\n","We utilized bar charts to illustrate the distribution of rented bike counts with other categorical variables."],"metadata":{"id":"aJRCwT6DYrdO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"F6T5p64dYrdO"}},{"cell_type":"markdown","source":["From above bar charts we got insights:\n","\n","In hour vs rented bike chart there is high demand in the morning 8'o clock and evening 18'o clock\n","From season vs rented bike chart there is more demand in summer and less demand in winter.\n","There is high demand on working days.\n","From month chart we know that there is high demand in month of june.Answer Here"],"metadata":{"id":"Xx8WAJvtYrdO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"y-Ehk30pYrdP"}},{"cell_type":"markdown","source":["Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","Absolutely, these insights are likely to have a positive impact on the business. By analyzing demand based on categorical variables, we can discern when bike demand is highest, allowing us to focus more resources on those specific periods.Answer Here"],"metadata":{"id":"jLNxxz7MYrdP"}},{"cell_type":"markdown","source":["#### Chart - 5"],"metadata":{"id":"bamQiAODYuh1"}},{"cell_type":"code","source":["#ploting line graph\n","# group by Hrs and get average Bikes rented, and precent change\n","avg_rent_hrs = bike_df_1.groupby('Hour')['Rented_Bike_Count'].mean()\n","\n","# plot average rent over time(hrs)\n","plt.figure(figsize=(12,6))\n","sns.lineplot(data=avg_rent_hrs, marker='o')\n","plt.title('Average bike rented per hour')\n","# a=avg_rent_hrs.plot(legend=True,marker='o',title=\"Average Bikes Rented Per Hr\")\n","# a.set_xticks(range(len(avg_rent_hrs)))\n","# a.set_xticklabels(avg_rent_hrs.index.tolist(), rotation=85)"],"metadata":{"id":"TIJwrbroYuh3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"QHF8YVU7Yuh3"}},{"cell_type":"markdown","source":["A line plot, also referred to as a line chart or line graph, is a method to visualize the trend of a single variable over time. It connects a series of data points with a line to illustrate how the value of the variable changes over time.\n","\n","Line plots are valuable as they swiftly and clearly display trends and patterns in the data, particularly showcasing how a variable changes over a period of time. They are also useful for comparing the trends of multiple variables.\n","\n","We employed a line plot to observe the distribution of rented bike demand over a 24-hour period.Answer Here."],"metadata":{"id":"dcxuIMRPYuh3"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"GwzvFGzlYuh3"}},{"cell_type":"markdown","source":["From above line plot we can clearly see that there is high demand in the morning and in the evening.Answer Here"],"metadata":{"id":"uyqkiB8YYuh3"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"qYpmQ266Yuh3"}},{"cell_type":"markdown","source":["Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","Absolutely, the insight gleaned indicates a high demand in the morning and evening, suggesting that the business should prioritize and focus on meeting the demand during these specific time slots.Answer Here"],"metadata":{"id":"_WtzZ_hCYuh4"}},{"cell_type":"markdown","source":["#### Chart - 6"],"metadata":{"id":"OH-pJp9IphqM"}},{"cell_type":"code","source":["# Chart - 6 visualization code\n","for i in cat_var:\n","  if i == 'hour':\n","    continue\n","  else:\n","    fig, ax = plt.subplots(figsize=(12,5))\n","    sns.pointplot(data=bike_df_1, x='Hour', y='Rented_Bike_Count', hue=i, ax=ax)\n","    plt.title('Hourly bike demand broken down based on the attribute: '+i)\n","    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left',title=i)\n","    plt.show()"],"metadata":{"id":"kuRf4wtuphqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"bbFf2-_FphqN"}},{"cell_type":"markdown","source":["A line plot, also referred to as a line chart or line graph, is a method to visualize the trend of a single variable over time, connecting a series of data points with a line to illustrate changes over time.\n","\n","Line plots are valuable for swiftly and clearly displaying trends and patterns in the data, particularly showcasing how a variable changes over a period of time. They are also useful for comparing the trends of multiple variables.\n","\n","We utilized a line plot, drawing multiple lines on charts, to illustrate the demand for rented bikes throughout the day based on other categorical variables.Answer Here."],"metadata":{"id":"loh7H2nzphqN"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"_ouA3fa0phqN"}},{"cell_type":"markdown","source":["From above line plots we see that :\n","\n","In winter season there is no significant demand even in the morning or in the evening.\n","On the functional day (i.e No Holiday) there is spike in morning and in evening, but that is not there on Holidays.\n","Around 3 months in winter season (i.e December, January & February) there is low demand.\n","On weekend almost throught the day there is demand.Answer Here"],"metadata":{"id":"VECbqPI7phqN"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"Seke61FWphqN"}},{"cell_type":"markdown","source":["Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","Yes, from this analysis we figure out some key factors such as high demand in morning and evening slot in all the seasons.Answer Here"],"metadata":{"id":"DW4_bGpfphqN"}},{"cell_type":"markdown","source":["#### Chart - 7"],"metadata":{"id":"PIIx-8_IphqN"}},{"cell_type":"code","source":["#plot for rented bike count seasonly\n","sns.catplot(x='Seasons',y='Rented_Bike_Count',data=bike_df_1)"],"metadata":{"id":"lqAIGUfyphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t27r6nlMphqO"}},{"cell_type":"markdown","source":["The catplot function is utilized to create a categorical plot, which is employed to visualize the distribution of a categorical variable. These plots can illustrate how a variable is related to a categorical variable and can also compare the distribution of multiple categorical variables.\n","\n","We used the catplot to observe the distribution of rented bikes based on the \"season\" column.Answer Here."],"metadata":{"id":"iv6ro40sphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"r2jJGEOYphqO"}},{"cell_type":"markdown","source":["From above catplot we got know that:\n","\n","There is low demand in winter\n","Also in all seasons upto the 2500 bike counts distribution is seen dense.Answer Here"],"metadata":{"id":"Po6ZPi4hphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"b0JNsNcRphqO"}},{"cell_type":"markdown","source":["Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","Yes, from this catplot we know that there is high bike count upto the 2500 so, above that there maybe outliers present. business needs to evaluate that.Answer Here"],"metadata":{"id":"xvSq8iUTphqO"}},{"cell_type":"markdown","source":["#### Chart - 8"],"metadata":{"id":"BZR9WyysphqO"}},{"cell_type":"code","source":["# Chart - 8 visualization code\n","\n","# Grouping by season and summing the rented bike count\n","season_counts = bike_df_1.groupby('Seasons')['Rented_Bike_Count'].sum()\n","\n","# Plotting the pie chart\n","plt.figure(figsize=(5, 5))\n","plt.pie(season_counts, labels=season_counts.index, autopct='%1.1f%%')\n","plt.title(\"Distribution of rented bikes by season\", fontsize=20)\n","plt.show()"],"metadata":{"id":"TdPTWpAVphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"jj7wYXLtphqO"}},{"cell_type":"markdown","source":["Pie charts are commonly employed to display the proportions of a whole, particularly useful for presenting data that has been calculated as a percentage of the whole.\n","\n","In this case, we utilized a pie chart to illustrate the percentage distribution of rented bikes based on different seasons.Answer Here."],"metadata":{"id":"Ob8u6rCTphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"eZrbJ2SmphqO"}},{"cell_type":"markdown","source":["From above pie chart:\n","\n","In year data season summer contributes around 37% then autumn around 29%\n","Lowest demand in winter, it contributes around only 8%Answer Here"],"metadata":{"id":"mZtgC_hjphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"rFu4xreNphqO"}},{"cell_type":"markdown","source":["Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","This insights only tell about percentage contribution of year data of season varible, which clearly gave indication about demand."],"metadata":{"id":"ey_0qi68phqO"}},{"cell_type":"markdown","source":["#### Chart - 9"],"metadata":{"id":"YJ55k-q6phqO"}},{"cell_type":"code","source":["# Select only numeric columns for correlation calculation\n","numeric_columns = bike_df_1.select_dtypes(include=[np.number])\n","\n","corr = numeric_columns.corr()\n","mask = np.triu(np.ones_like(corr, dtype=bool))\n","\n","with sns.axes_style(\"white\"):\n","    f, ax = plt.subplots(figsize=(18, 7))\n","    ax = sns.heatmap(corr, mask=mask, vmin=-1, vmax=1, annot=True, cmap=\"viridis\")"],"metadata":{"id":"B2aS4O1ophqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"gCFgpxoyphqP"}},{"cell_type":"markdown","source":["The correlation coefficient serves as a measure of the strength and direction of a linear relationship between two variables. A correlation matrix is employed to summarize the relationships among a set of variables, serving as a crucial tool for data exploration and for selecting which variables to include in a model. The correlation range is between -1 and 1.\n","\n","To understand the correlation between all the variables, along with the correlation coefficients, we utilized a correlation heatmap.Answer Here."],"metadata":{"id":"TVxDimi2phqP"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"OVtJsKN_phqQ"}},{"cell_type":"markdown","source":["From above correlation map we can clearly see that:\n","\n","There is high multicolinearity between independent variable (i.e temperature & dew point temp, humidity & dew point temp, weekend & day of week).\n","There is correlation of temperature, hour, dew point temp & solar radiation with dependent variable rented bike.\n","Other than that we didnt see any correlation.Answer Here"],"metadata":{"id":"ngGi97qjphqQ"}},{"cell_type":"markdown","source":["#### Chart - 10"],"metadata":{"id":"U2RJ9gkRphqQ"}},{"cell_type":"code","source":["# Pair Plot\n","sns.pairplot(bike_df_1)\n","plt.show()"],"metadata":{"id":"GM7a4YP4phqQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"1M8mcRywphqQ"}},{"cell_type":"markdown","source":["A pairplot, also known as a scatterplot matrix, is a visualization that allows you to visualize the relationships between all pairs of variables in a dataset. It is a useful tool for data exploration because it allows you to quickly see how all of the variables in a dataset are related to one another.\n","\n","Thus, we used pair plot to analyse the patterns of data and realationship between the features. It's exactly same as the correlation map but here you will get the graphical representation.Answer Here."],"metadata":{"id":"8agQvks0phqQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"tgIPom80phqQ"}},{"cell_type":"markdown","source":["From above pair plot we got to know that, there is not clear linear relationship between variables. other than dew point temp, temperature & solar radiation there is not any reationship.Answer Here"],"metadata":{"id":"Qp13pnNzphqQ"}},{"cell_type":"markdown","source":["Pays little attention to the skewness of our numerical features"],"metadata":{"id":"K5IFOAfR175z"}},{"cell_type":"code","source":["bike_df = bike_df_1.copy()"],"metadata":{"id":"MdKTFV692CRl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# separate numerical features from the dataframe\n","numeric_features= bike_df.select_dtypes(exclude=['object','category'])\n"],"metadata":{"id":"X-c7GnNJ2GPC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# printing displots to analyze the distribution of all numerical features\n","\n","n=1\n","plt.figure(figsize=(15,10))\n","for i in numeric_features.columns:\n","  plt.subplot(3,3,n)\n","  n=n+1\n","  sns.distplot(bike_df[i])\n","  plt.title(i)\n","  plt.tight_layout()"],"metadata":{"id":"NvFAyblY2KcV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Right skewed columns are**\n","Rented Bike Count (Its also our Dependent variable), Wind speed (m/s), Solar Radiation (MJ/m2), Rainfall(mm), Snowfall (cm),\n","\n","# **Left skewed columns are**\n","Visibility (10m), Dew point temperature(°C)"],"metadata":{"id":"uMgMdYKe2b1S"}},{"cell_type":"markdown","source":["## ***5. Hypothesis Testing***"],"metadata":{"id":"g-ATYxFrGrvw"}},{"cell_type":"markdown","source":["### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."],"metadata":{"id":"Yfr_Vlr8HBkt"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"-7MS06SUHkB-"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 1"],"metadata":{"id":"8yEUt7NnHlrM"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"tEA2Xm5dHt1r"}},{"cell_type":"markdown","source":["Based on above chart experiments we have noticed that our dependent variable does not seems to normally distributed so we have made hypothetical assumption that our data is normally distributed and for that we have decided to do statistical analysis.\n","\n","Normality test\n","\n","for normality test we decided\n","\n","Null hypothesis : Data is normally distributed\n","Alternate hypothesis : Data is not normally distributedAnswer Here.\n","2. Perform an appropriate statistical test."],"metadata":{"id":"HI9ZP0laH0D-"}},{"cell_type":"code","source":["bike_df.columns"],"metadata":{"id":"TmO5MmR93GkI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.stats import shapiro\n","\n","test_data = bike_df['Rented_Bike_Count']\n","\n","# Perform Shapiro-Wilk test\n","stats, p = shapiro(test_data)\n","\n","print('Test Statistic = %.2f, p-value = %.3f' % (stats, p))\n","\n","# Interpret the results\n","if p <= 0.05:\n","    print(\"Reject the null hypothesis: Data is not normally distributed\")\n","else:\n","    print(\"Fail to reject the null hypothesis: Data is normally distributed\")"],"metadata":{"id":"yco-PKt63LIR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Which statistical test have you done to obtain P-Value?**\n","\n","We use Shapiro-wilk statistical test to obtain the p-value and we got very less p-value which is less than 0.05.\n","\n","**Why did you choose the specific statistical test?**\n","\n","The Shapiro-Wilk test is used to test the normality of a sample. The test checks whether the sample data fits a normal distribution, which is often assumed for statistical analysis. The test results can help determine if the data should be transformed or if non-parametric statistical methods should be used instead of traditional parametric methods."],"metadata":{"id":"kmOSDApk9oPa"}},{"cell_type":"code","source":["import statsmodels.api as sm\n","import matplotlib.pyplot as plt\n","\n","# Generate the Q-Q plot\n","fig, ax = plt.subplots(figsize=(7,4))\n","sm.qqplot(test_data, line='s', ax=ax)\n","ax.set_title('Q-Q Plot for Rented Bike Count')\n","plt.show()"],"metadata":{"id":"erPEIV919_Om"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**2. Effect of Weather Conditions on Bike Demand:**\n","\n","Null Hypothesis (H0): There is no significant relationship between weather conditions (temperature, humidity, wind speed, visibility, dew point temperature, solar radiation, rainfall, snowfall) and the number of rented bikes.\n","\n","Alternative Hypothesis (H1): Weather conditions significantly influence the number of rented bikes.\n","\n","Test: Multiple linear regression analysis with weather variables as independent variables and rented bike count as the dependent"],"metadata":{"id":"LAFQPx-v-Edo"}},{"cell_type":"code","source":["import pandas as pd\n","import statsmodels.api as sm\n","\n","# Load your data into a DataFrame named bike_df\n","\n","# Define independent variables (weather conditions) and dependent variable (rented bike count)\n","X = bike_df[['Temperature', 'Humidity', 'Wind_speed', 'Visibility',\n","             'Solar_Radiation', 'Rainfall', 'Snowfall']]\n","y = bike_df['Rented_Bike_Count']\n","\n","# Add a constant term to the independent variables (for intercept)\n","X = sm.add_constant(X)\n","\n","# Fit the multiple linear regression model\n","model = sm.OLS(y, X).fit()\n","\n","# Determine significance level (alpha)\n","alpha = 0.05\n","\n","# Get p-values for each coefficient\n","p_values = model.pvalues.drop('const')  # Drop constant term\n","significant_vars = p_values[p_values < alpha]\n","\n","# Print significant variables\n","if len(significant_vars) > 0:\n","    print(\"Significant weather conditions influencing bike demand:\")\n","    print(significant_vars)\n","    print(\"Reject null hypothesis: Weather conditions significantly influence the number of rented bikes.\")\n","else:\n","    print(\"No significant weather conditions influencing bike demand.\")\n","    print(\"Accept null hypothesis: There is no significant relationship between weather conditions and the number of rented bikes.\")\n","\n",""],"metadata":{"id":"FkscmKPE-f5Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**3. Impact of Time Factors on Bike Demand:**\n","\n","Null Hypothesis (H0): There is no significant difference in bike demand across different hours of the day.\n","\n","Alternative Hypothesis (H1): Bike demand varies significantly across different hours of the day.\n","\n","Test: ANOVA (Analysis of Variance) to compare mean bike rental counts across different hours."],"metadata":{"id":"VesuxBw5-nDK"}},{"cell_type":"code","source":["from scipy.stats import f_oneway\n","\n","# Split data by hour\n","hourly_data = [group['Rented_Bike_Count'] for _, group in bike_df.groupby('Hour')]\n","\n","# Perform one-way ANOVA\n","f_statistic, p_value = f_oneway(*hourly_data)\n","\n","# Determine significance level (alpha)\n","alpha = 0.05\n","\n","# Print result of ANOVA\n","if p_value < alpha:\n","    print(\"Reject null hypothesis: Bike demand varies significantly across different hours of the day.\")\n","else:\n","    print(\"Accept null hypothesis: There is no significant difference in bike demand across different hours of the day.\")\n","\n",""],"metadata":{"id":"5sKswTZi-x5u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**4. Holiday and Seasonal Effects on Bike Demand:**\n","\n","Null Hypothesis (H0): There is no significant difference in bike demand between holidays and non-holidays, as well as across different seasons.\n","\n","Alternative Hypothesis (H1): Bike demand varies significantly between holidays and non-holidays, as well as across different seasons.\n","\n","Test: two-sample t-test to analyze the holiday and seasonal effects on bike demand"],"metadata":{"id":"0_yQo1RQ_A-Y"}},{"cell_type":"code","source":["from scipy.stats import ttest_ind\n","\n","# Split data by holiday and non-holiday\n","holiday_data = bike_df[bike_df['Holiday'] == 'Holiday']['Rented_Bike_Count']\n","non_holiday_data = bike_df[bike_df['Holiday'] == 'No Holiday']['Rented_Bike_Count']\n","\n","# Perform two-sample t-test\n","t_statistic, p_value = ttest_ind(holiday_data, non_holiday_data)\n","\n","# Determine significance level (alpha)\n","alpha = 0.05\n","\n","# Print result of two-sample t-test\n","if p_value < alpha:\n","    print(\"Reject null hypothesis: Bike demand varies significantly between holiday and non-holiday periods.\")\n","else:\n","    print(\"Accept null hypothesis: There is no significant difference in bike demand between holiday and non-holiday periods.\")\n","\n",""],"metadata":{"id":"niM0rvj4_MAo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value"],"metadata":{"id":"oZrfquKtyian"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Normalise Rented_Bike_Count column data**\n","The data normalization (also referred to as data pre-processing) is a basic element of data mining. It means transforming the data, namely converting the source data in to another format that allows processing data effectively. The main purpose of data normalization is to minimize or even exclude duplicated data"],"metadata":{"id":"eP8i3wYq_VBI"}},{"cell_type":"code","source":["#Distribution plot of Rented Bike Count\n","plt.figure(figsize=(10,6))\n","plt.xlabel('Rented_Bike_Count')\n","plt.ylabel('Density')\n","ax=sns.distplot(bike_df['Rented_Bike_Count'],hist=True ,color=\"y\")\n","ax.axvline(bike_df['Rented_Bike_Count'].mean(), color='magenta', linestyle='dashed', linewidth=2)\n","ax.axvline(bike_df['Rented_Bike_Count'].median(), color='black', linestyle='dashed', linewidth=2)\n","plt.show()"],"metadata":{"id":"h7Xqzx8f_duP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**The above graph shows that, Rented Bike Count has moderate right skewness. Since the assumption of linear regression is that 'the distribution of dependent variable has to be normal', so we should perform some operation to make it normal.**"],"metadata":{"id":"7sSXsvHh_mr-"}},{"cell_type":"code","source":["from scipy.stats import boxcox\n","from scipy.stats import yeojohnson\n","\n","# Define the data\n","test_data = bike_df['Rented_Bike_Count']\n","# test_data = bike_df['Wind_speed']\n","\n","# Apply different transformations\n","transformations = {\n","    'Original': test_data,\n","    'Reciprocal Transformation': np.reciprocal(test_data),\n","    'Log Transformation': np.log(test_data),\n","    'Log1p Transformation': np.log1p(test_data),\n","    'Square Root Transformation': np.sqrt(test_data),\n","}\n","\n","# Apply Yeo-Johnson transformation if available\n","try:\n","    transformed_data, lambda_val = yeojohnson(test_data)\n","    transformations['Yeo-Johnson Transformation'] = transformed_data\n","except Exception as e:\n","    print(\"Warning:\", e)\n","\n","# Apply Box-Cox transformation if data is positive\n","if (test_data > 0).all():\n","    transformed_data, lambda_val = boxcox(test_data)\n","    transformations['Box-Cox Transformation'] = transformed_data\n","\n","# Plot the distributions\n","plt.figure(figsize=(16, 12))\n","for i, (transformation, data) in enumerate(transformations.items(), 1):\n","    plt.subplot(3, 3, i)\n","    sns.histplot(data, kde=True, stat='density')\n","    plt.title(transformation)\n","    plt.xlabel('Rented Bike Count')\n","    plt.ylabel('Density')\n","\n","    # Calculate skewness, mean, and median\n","    skew = np.mean((data - np.mean(data))**3) / np.mean((data - np.mean(data))**2)**(3/2)\n","    mean = np.mean(data)\n","    median = np.median(data)\n","\n","    # Print skewness, mean, and median below each histogram\n","    plt.text(0.5, 0.75, f'Skewness: {skew:.2f}', transform=plt.gca().transAxes)\n","    plt.text(0.5, 0.65, f'Mean: {mean:.2f}', transform=plt.gca().transAxes)\n","    plt.text(0.5, 0.55, f'Median: {median:.2f}', transform=plt.gca().transAxes)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"TwZuEv7c_zLi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transformed_y, lambda_val = yeojohnson(test_data)"],"metadata":{"id":"tEa5SbBMCgvT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate the Q-Q plot\n","fig, ax = plt.subplots(figsize=(7,4))\n","sm.qqplot(transformed_y, line='s', ax=ax)\n","ax.set_title('Q-Q Plot for Rented Bike Count')\n","plt.show()"],"metadata":{"id":"UPvc2IvSCkG6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lambda_val"],"metadata":{"id":"O8X0gpy4CrIh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot the distribution\n","plt.figure(figsize=(8, 4))\n","sns.histplot(transformed_y, kde=True, stat='density')\n","plt.xlabel(test_data.name)\n","plt.title(f'Distribution of {test_data.name} (after Yeo-Johnson transformation)')\n","plt.axvline(np.mean(transformed_y), color='magenta', linewidth=2, label='Mean')\n","plt.axvline(np.median(transformed_y), color='cyan', linestyle='dashed', linewidth=2, label='Median')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"wU9ZFAbjCxRG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **We plot distribution plot and also we did normality test using shapiro wilk and we have found that our data is not normally distributed it needs transformation.**\n","So, first we have calculate the skewness value and we have found that rented bike attribute is positively skewed so we used log transfomation but it affected negatively\n","\n","So, we finally used Yeo-Johnson transformation & now our data looks normally distrubuted & skewness is also reduced.\n","\n"],"metadata":{"id":"Vv-GQleQC16o"}},{"cell_type":"markdown","source":["**Finding Outliers and treatment**"],"metadata":{"id":"hR9GytNGC_hs"}},{"cell_type":"code","source":["# Boxplot for Rented bike Count to check outliers\n","plt.figure(figsize=(10,6))\n","\n","plt.ylabel('Rented_Bike_Count')\n","sns.boxplot(x=transformed_y)\n","plt.show()"],"metadata":{"id":"wpKScoHPDDVM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# outliers treatments\n","bike_df.loc[bike_df['Rainfall']>=4,'Rainfall']= 4\n","bike_df.loc[bike_df['Solar_Radiation']>=2.5,'Solar_Radiation']=2.5\n","bike_df.loc[bike_df['Snowfall']>2,'Snowfall']= 2\n","bike_df.loc[bike_df['Wind_speed']>=4,'Wind_speed']= 4"],"metadata":{"id":"QlS3q2ViDJi3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Checking of Correlation between variables**\n","\n","**Checking in OLS Model**\n","\n","Ordinary least squares (OLS) regression is a statistical method of analysis that estimates the relationship between one or more independent variables and a dependent variable"],"metadata":{"id":"9Fd5SV0cDNzR"}},{"cell_type":"code","source":["#import the module\n","#assign the 'x','y' value\n","import statsmodels.api as sm\n","X = bike_df[[ 'Temperature','Humidity',\n","       'Wind_speed', 'Visibility','Dew_point_temperature',\n","       'Solar_Radiation', 'Rainfall', 'Snowfall']]\n","Y = bike_df['Rented_Bike_Count']\n","bike_df.head()"],"metadata":{"id":"hkcAqUfkDcJE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#add a constant column\n","X = sm.add_constant(X)\n","X.head()"],"metadata":{"id":"cbYawHfFDf3Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## fit an OLS model\n","\n","model= sm.OLS(Y, X).fit()\n","model.summary()"],"metadata":{"id":"Qt8lzjWcDmD8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**R sqauare and Adj Square are near to each other. 40% of variance in the Rented Bike count is explained by the model.**\n","\n","**For F statistic , P value is less than 0.05 for 5% levelof significance.**\n","\n","**P value of dew point temp and visibility are very high and they are not significant.**\n","\n","**Omnibus tests the skewness and kurtosis of the residuals. Here the value of Omnibus is high., it shows we have skewness in our data**.\n","\n","**The condition number is large, 3.11e+04. This might indicate that there are strong multicollinearity or other numerical problems**\n","\n","**Durbin-Watson tests for autocorrelation of the residuals. Here value is less than 0.5. We can say that there exists a positive auto correlation among the variables.**"],"metadata":{"id":"eC_pXOJqDt2M"}},{"cell_type":"code","source":["X.corr()"],"metadata":{"id":"IQyrkMicEIpv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**From the OLS model we find that the 'Temperature' and 'Dew_point_temperature' are highly correlated so we need to drop one of them.**\n","\n","**For droping them we check the (P>|t|) value from above table and we can see that the 'Dew_point_temperature' value is higher so we need to drop Dew_point_temperature column**\n","\n","For clarity, we use visualisation i.e heatmap in next step"],"metadata":{"id":"mvMR7xmxENIn"}},{"cell_type":"markdown","source":["# **Heatmap**\n","**A correlation Heatmap is a type of graphical representation that displays the correlation matrix, which helps to determine the correlation between different variables.**"],"metadata":{"id":"OUh-L8r3EikA"}},{"cell_type":"code","source":["#checking correlation using heatmap\n","\n","plt.figure(figsize=(10,5))\n","sns.heatmap(X.corr(),cmap='PiYG',annot=True)"],"metadata":{"id":"5LJnqWSbEq2S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**We can observe on the heatmap that on the target variable line, the most positively correlated variables to the rent are:**\n","\n","the temperature\n","\n","the dew point temperature\n","\n","the solar radiation\n","\n","**And most negatively correlated variables are:**\n","\n","humidity\n","\n","rainfall"],"metadata":{"id":"Ps5AJFgoEw5d"}},{"cell_type":"markdown","source":["**From the above correlation heatmap, We see that there is a positive correlation between columns 'Temperature' and 'Dew point temperature' i.e 0.91 so even if we drop this column then it won't affect the outcome of our analysis. And they have the same variations, so we can drop the column 'Dew point temperature(°C)'.**"],"metadata":{"id":"tlpOXOe0E_uY"}},{"cell_type":"code","source":["# drop the Dew point temperature column\n","bike_df=bike_df.drop(['Dew_point_temperature'],axis=1)\n",""],"metadata":{"id":"uEDuIV2yFH8Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bike_df.info()"],"metadata":{"id":"Bi7LBMnuFLiK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Feature Engineering & Data Pre-processing**"],"metadata":{"id":"6LKblKZeFRBI"}},{"cell_type":"markdown","source":["\"\"\" Note:\n","\n","Certain code blocks have been commented out because a custom function has been created to handle various preprocessing steps, including train-test split, transformation, one-hot encoding, scaling, and the use of function transformers and pipelines.\n","\n","Instead of running individual code blocks, you can directly execute the custom function, which performs all these steps in one go for efficiency and convenience. \"\"\""],"metadata":{"id":"OGQVCgc7FTCL"}},{"cell_type":"markdown","source":["**Feature Selection**"],"metadata":{"id":"2AHzHLeVFZtI"}},{"cell_type":"code","source":["#remove multicollinearity by using VIF technique\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","def calc_vif(X):\n","\n","    # Calculating VIF\n","    vif = pd.DataFrame()\n","    vif[\"variables\"] = X.columns\n","    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n","\n","    return(vif)"],"metadata":{"id":"rARJUs56FcAv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["numeric_features = bike_df.select_dtypes(include='number').columns.tolist()\n","numeric_features.remove('Rented_Bike_Count')"],"metadata":{"id":"1LvxTeZAFib6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["numeric_features"],"metadata":{"id":"3azEao_4FkAJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bike_df[numeric_features].head()"],"metadata":{"id":"J6za85w2FpJK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["calc_vif(bike_df[[i for i in bike_df[numeric_features]]])"],"metadata":{"id":"gAdq8XuNFtXx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Create the dummy variables**\n","\n","**A dataset may contain various type of values, sometimes it consists of categorical values. So, in-order to use those categorical value for programming efficiently we create dummy variables.**"],"metadata":{"id":"MQREvg85GGtw"}},{"cell_type":"markdown","source":["# **One Hot Encoding**\n","**A one hot encoding allows the representation of categorical data to be more expressive. Many machine learning algorithms cannot work with categorical data directly. The categories must be converted into numbers. This is required for both input and output variables that are categorical.**"],"metadata":{"id":"OOFIi8XRGQmj"}},{"cell_type":"code","source":["# bike_df_copy = bike_df.copy()\n","\n","# # Define the one-hot encoding function\n","# def one_hot_encoding(data, column):\n","#     # Apply one-hot encoding to the specified column\n","#     encoded_column = pd.get_dummies(data[column], prefix=column, drop_first=True)\n","#     # Convert encoded values to 1 or 0\n","#     encoded_column = encoded_column.astype(int)\n","#     # Concatenate the encoded column with the original DataFrame\n","#     data = pd.concat([data, encoded_column], axis=1)\n","#     # Drop the original column after encoding\n","#     data = data.drop([column], axis=1)\n","#     return data\n","\n","# # Apply one-hot encoding to each categorical feature\n","# for col in categorical_features:\n","#     bike_df_copy = one_hot_encoding(bike_df_copy, col)\n","\n","# # Display the first few rows of the transformed DataFrame\n","# bike_df_copy.head()\n"],"metadata":{"id":"uRBWGGXqGWsR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **6. Data Scaling**"],"metadata":{"id":"lOW20FngGule"}},{"cell_type":"code","source":["# Assuming bike_df is your DataFrame\n","categorical_features = bike_df.select_dtypes(include=['object','category']).columns.tolist()\n","\n","categorical_feature_indices = [bike_df.drop(columns = 'Rented_Bike_Count').columns.get_loc(col) for col in categorical_features]\n","\n","print(categorical_feature_indices)\n"],"metadata":{"id":"2C_bLo7yG0SW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["numeric_features_indices = [bike_df.drop(columns='Rented_Bike_Count').columns.get_loc(col) for col in numeric_features]\n","\n","print(numeric_features_indices)"],"metadata":{"id":"GkvB3eCwG4jO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **8. Data Splitting**"],"metadata":{"id":"UsxpY2W1G8f5"}},{"cell_type":"code","source":["# from sklearn.model_selection import train_test_split\n","# X = df.drop(columns=['Rented_Bike_Count'])\n","# y = df['Rented_Bike_Count']\n","# # Apply Yeojohnson transformation to the target variable\n","# y_transformed = yeojohnson_transform(y)\n","\n","# # Split the data into training and testing sets\n","# X_train, X_test, y_train, y_test = train_test_split(X, y_transformed, test_size=0.25, random_state=42)\n","\n","# # Define transformers for numerical and categorical features\n","# numeric_transformer = Pipeline(steps=[\n","#     ('scaler', StandardScaler())])\n","\n","# categorical_transformer = Pipeline(steps=[\n","#     ('onehot', OneHotEncoder(sparse=False, drop='first'))])\n","\n","# # Define the ColumnTransformer\n","# transformer = ColumnTransformer(transformers=[\n","#     ('tnf1', categorical_transformer, categorical_feature_indices),\n","#     ('scaler', numeric_transformer, numeric_features_indices)\n","# ], remainder='passthrough')\n","\n","# # Append regressor to preprocessing pipeline\n","# rf_pipeline = Pipeline(steps=[\n","#     ('preprocessor', transformer),\n","#     ('regressor', LinearRegression())\n","# ])\n","\n","# # Fit the pipeline on the training data\n","# rf_pipeline.fit(X_train, y_train)\n","\n","# # Evaluate the pipeline on the testing data\n","# score = rf_pipeline.score(X_test, y_test)\n","\n","# print(\"Pipeline Score:\", score)"],"metadata":{"id":"MlzGBE36G_KN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Assuming 'transformer' is your ColumnTransformer object\n","# transformers = transformer.named_transformers_\n","\n","# # Print the transformations applied by the categorical_transformer\n","# print(\"Categorical Transformer:\")\n","# print(transformers['tnf1'])\n","\n","# # Print the transformations applied by the numeric_transformer\n","# print(\"Numeric Transformer:\")\n","# print(transformers['scaler'])\n","\n","# # Print the remainder setting\n","# print(\"Remainder:\", transformer.remainder)\n",""],"metadata":{"id":"UgpRPbNbHGrA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **What data splitting ratio have you used and why?**\n","To train the model we have split the data into train and test using train_test_split method\n","\n","We have split 80% of our data into train and 20% into test."],"metadata":{"id":"z1Z1rhLvHJaG"}},{"cell_type":"markdown","source":["Before, fitting any model it is a rule of thumb to split the dataset into a training and test set. This means some proportions of the data will go into training the model and some portion will be used to evaluate how our model is performing on any unseen data. The proportions may vary from 60:40, 70:30, 75:25 depending on the person but mostly used is 80:20 for training and testing respectively. In this step we will split our data into training and testing set using scikit learn library.\n","\n","I have done this inside a function"],"metadata":{"id":"wixP2ObCHfQa"}},{"cell_type":"code","source":["\n","df  = bike_df.copy()"],"metadata":{"id":"pebFVgd8HiN5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Function to apply train test split , transformation , one hot encoding , scalling**"],"metadata":{"id":"67f9RaApHoGs"}},{"cell_type":"code","source":["def transform_data(X, y):\n","    # Apply Yeojohnson transformation to the target variable\n","    y_transformed, lambda_ = yeojohnson(y)\n","\n","    # Split the data into training and testing sets\n","    X_train, X_test, y_train, y_test = train_test_split(X, y_transformed, test_size=0.2, random_state=42)\n","\n","    # Define transformers for numerical and categorical features\n","    numeric_transformer = Pipeline(steps=[\n","        ('scaler', StandardScaler())\n","    ])\n","\n","    categorical_transformer = Pipeline(steps=[\n","        ('onehot', OneHotEncoder(sparse=False, drop='first', handle_unknown='ignore'))  # Ignore unknown categories\n","    ])\n","\n","    # Define feature indices properly\n","       # Update this with the indices of your numerical features\n","\n","    preprocessor = ColumnTransformer(transformers=[\n","        ('categorical', categorical_transformer, categorical_feature_indices),\n","        ('numeric', numeric_transformer, numeric_features_indices)\n","    ], remainder='passthrough')\n","\n","    # Define the pipeline\n","    pipeline = Pipeline(steps=[\n","        ('preprocessor', preprocessor),\n","    ])\n","\n","    # Fit the pipeline on the training data\n","    pipeline.fit(X_train, y_train)\n","\n","    # Transform the training and testing data\n","    X_train_transformed = pipeline.transform(X_train)\n","    X_test_transformed = pipeline.transform(X_test)\n","\n","    # Get feature names after transformation\n","    categorical_feature_names = pipeline.named_steps['preprocessor'].transformers_[0][1]['onehot'] \\\n","        .get_feature_names_out(input_features=X.columns[categorical_feature_indices])\n","    feature_names = np.concatenate((categorical_feature_names, X.columns[numeric_features_indices]))\n","\n","    # Create DataFrame for transformed X_train data\n","    X_train_transformed_df = pd.DataFrame(X_train_transformed, columns=feature_names)\n","\n","    return X_train_transformed, X_test_transformed, y_train, y_test, X_train_transformed_df\n","\n","X = df.drop(columns=['Rented_Bike_Count'])\n","y = df['Rented_Bike_Count']\n","X_train_transformed, X_test_transformed, y_train, y_test, X_train_transformed_df = transform_data(X, y)"],"metadata":{"id":"XZ71g-fHHwrq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**The function transform_data preprocesses the input data for machine learning tasks by:**\n","\n","Applying the Yeojohnson transformation to the target variable.\n","\n","Splitting the data into training and testing sets.\n","\n","Standardizing numerical features and performing one-hot encoding with handling of unknown categories for categorical features.\n","\n","Constructing a pipeline to apply these transformations.\n","\n","Fitting the pipeline to the training data and transforming both the training and testing sets accordingly.\n","\n","Providing transformed datasets along with feature names and a DataFrame representing the transformed training data."],"metadata":{"id":"nMJGQi69H2bz"}},{"cell_type":"markdown","source":["# **Function to aaply preprocessing and run model**"],"metadata":{"id":"6EFiUyrFIDrK"}},{"cell_type":"code","source":["models_test_res_list = []\n","models_train_res_list = []\n","\n","# Define a function to calculate adjusted R-squared\n","def adjusted_r2_score(y_true, y_pred, n_samples, n_features):\n","    r2 = r2_score(y_true, y_pred)\n","    adjusted_r2 = 1 - (1 - r2) * ((n_samples - 1) / (n_samples - n_features - 1))\n","    return adjusted_r2\n","\n","def evaluate_model(model_name, model, X, y):\n","    # Apply Yeojohnson transformation to the target variable\n","    y_transformed, lambda_ = yeojohnson(y)\n","\n","    # Split the data into training and testing sets\n","    X_train, X_test, y_train, y_test = train_test_split(X, y_transformed, test_size=0.2, random_state=42)\n","\n","    # Define transformers for numerical and categorical features\n","    numeric_transformer = Pipeline(steps=[\n","        ('scaler', StandardScaler())\n","    ])\n","\n","    categorical_transformer = Pipeline(steps=[\n","        ('onehot', OneHotEncoder(sparse=False, drop='first'))\n","    ])\n","\n","    # Define feature indices properly\n","    # categorical_feature_indices = ...\n","    # numeric_features_indices = ...\n","\n","    preprocessor = ColumnTransformer(transformers=[\n","        ('categorical', categorical_transformer, categorical_feature_indices),\n","        ('numeric', numeric_transformer, numeric_features_indices)\n","    ], remainder='passthrough')\n","\n","    # Define the pipeline\n","    pipeline = Pipeline(steps=[\n","        ('preprocessor', preprocessor),\n","        ('regressor', model)\n","    ])\n","\n","    # Fit the pipeline on the training data\n","    pipeline.fit(X_train, y_train)\n","\n","    # Obtain predictions on the training and testing datasets\n","    y_pred_train = pipeline.predict(X_train)\n","    y_pred_test = pipeline.predict(X_test)\n","\n","\n","    # Calculate evaluation metrics for training set\n","    mae_train = mean_absolute_error(y_train, y_pred_train)\n","    mse_train = mean_squared_error(y_train, y_pred_train)\n","    rmse_train = np.sqrt(mse_train)\n","    r2_train = r2_score(y_train, y_pred_train)\n","    adj_r2_train = adjusted_r2_score(y_train, y_pred_train, len(y_train), X.shape[1])\n","\n","    # Calculate evaluation metrics for the test set\n","    mae_test = mean_absolute_error(y_test, y_pred_test)\n","    mse_test = mean_squared_error(y_test, y_pred_test)\n","    rmse_test = np.sqrt(mse_test)\n","    r2_test = r2_score(y_test, y_pred_test)\n","    adj_r2_test = adjusted_r2_score(y_test, y_pred_test, len(y_test), X_test.shape[1])\n","\n","    # Print evaluation metrics\n","    print(\"Model:\", model_name)\n","    print(\"Mean Absolute Error (Train):\", mae_train)\n","    print(\"Mean Squared Error (Train):\", mse_train)\n","    print(\"Root Mean Squared Error (Train):\", rmse_train)\n","    print(\"R-squared (Train):\", r2_train)\n","    print(\"Adjusted R-squared (Train):\", adj_r2_train)\n","    print('-'*50)\n","    print(\"Mean Absolute Error (Test):\", mae_test)\n","    print(\"Mean Squared Error (Test):\", mse_test)\n","    print(\"Root Mean Squared Error (Test):\", rmse_test)\n","    print(\"R-squared (Test):\", r2_test)\n","    print(\"Adjusted R-squared (Test):\", adj_r2_test)\n","    print()\n","\n","    # Store the test set metrics in a dictionary\n","    test_metrics = {\n","    'Model': [model_name],\n","    'MAE': [mae_test],\n","    'MSE': [mse_test],\n","    'RMSE': [rmse_test],\n","    'R-squared': [r2_test],\n","    'Adjusted R-squared': [adj_r2_test]\n","    }\n","\n","    # Define train metrics\n","    train_metrics = {\n","        'Model': [model_name],\n","        'MAE': [mae_train],\n","        'MSE': [mse_train],\n","        'RMSE': [rmse_train],\n","        'R-squared': [r2_train],\n","        'Adjusted R-squared': [adj_r2_train]\n","    }\n","\n","    # Create DataFrames\n","    df_test = pd.DataFrame(test_metrics)\n","    df_train = pd.DataFrame(train_metrics)\n","\n","    models_test_res_list.append(df_test)\n","    models_train_res_list.append(df_train)\n","\n","\n","    # Return the evaluation metrics\n","    return df_test, df_train, y_pred_test, y_test, lambda_\n","\n","\n","\n","# Define the features and target variable\n","X = df.drop(columns=['Rented_Bike_Count'])\n","y = df['Rented_Bike_Count']"],"metadata":{"id":"o1tFp82EIM9G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**This function, evaluate_model, takes a model, data, and its corresponding target variable, then performs the following tasks:**\n","\n","It transforms the target variable using the Yeojohnson transformation.\n","\n","Splits the data into training and testing sets.\n","\n","Preprocesses the features using standard scaling for numerical features and one-hot encoding for categorical features.\n","\n","Fits a pipeline consisting of the preprocessor and the provided model to the training data.\n","\n","Calculates evaluation metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared, and Adjusted R-squared for both the training and testing sets.\n","\n","Prints and stores these evaluation metrics for further analysis.\n","\n","\n","Finally, it returns DataFrames containing the evaluation metrics for both the training and testing sets, as well as the predictions on the test set and the lambda value obtained from the Yeojohnson transformation."],"metadata":{"id":"rzS-a8KWITXK"}},{"cell_type":"markdown","source":["# **Function for feature importance**"],"metadata":{"id":"7OSDfxx9Ihbp"}},{"cell_type":"code","source":["def get_feature_importance(model, feature_names):\n","    if hasattr(model, 'feature_importances_'):\n","        importances = model.feature_importances_\n","        return dict(zip(feature_names, importances))\n","    else:\n","        return \"Feature importances are not available for this model.\""],"metadata":{"id":"J9m3a7mpIm8p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Function for gridsearch cv**"],"metadata":{"id":"_N25WbRXIrCU"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","\n","def perform_grid_search_cv(model_name, model, param_grid, X, y):\n","    # Apply Yeojohnson transformation to the target variable\n","    y_transformed, lambda_ = yeojohnson(y)\n","\n","    # Define transformers for numerical and categorical features\n","    numeric_transformer = Pipeline(steps=[\n","        ('scaler', StandardScaler())\n","    ])\n","\n","    categorical_transformer = Pipeline(steps=[\n","        ('onehot', OneHotEncoder(sparse=False, drop='first'))\n","    ])\n","\n","    # Define feature indices properly\n","    # categorical_feature_indices = ...\n","    # numeric_features_indices = ...\n","\n","    preprocessor = ColumnTransformer(transformers=[\n","        ('categorical', categorical_transformer, categorical_feature_indices),\n","        ('numeric', numeric_transformer, numeric_features_indices)\n","    ], remainder='passthrough')\n","\n","    # Define the pipeline\n","    pipeline = Pipeline(steps=[\n","        ('preprocessor', preprocessor),\n","        ('regressor', model)\n","    ])\n","\n","    # Define grid search CV\n","    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n","\n","    # Perform grid search CV\n","    grid_search.fit(X, y_transformed)\n","\n","    # Get the best model and best parameters\n","    best_model = grid_search.best_estimator_\n","    best_params = grid_search.best_params_\n","\n","    # Print the best parameters and return them\n","    print(\"Best parameters for\", model_name, \":\", best_params)\n","    return best_model, best_params"],"metadata":{"id":"QUklu0OhIxUx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The mean squared error (MSE) tells you how close a regression line is to a set of points. It does this by taking the distances from the points to the regression line (these distances are the “errors”) and squaring them. It’s called the mean squared error as you’re finding the average of a set of errors. The lower the MSE, the better the forecast.\n","\n","\n","MSE formula = (1/n) * Σ(actual – forecast)2 Where:\n","\n","\n","n = number of items,\n","\n","\n","Σ = summation notation,\n","\n","\n","Actual = original or observed y-value,\n","\n","\n","Forecast = y-value from regression.\n","\n","\n","Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors).\n","\n","\n","Mean Absolute Error (MAE) are metrics used to evaluate a Regression Model. ... Here, errors are the differences between the predicted values (values predicted by our regression model) and the actual values of a variable.\n","\n","\n","R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model.\n","\n","Formula for R-Squared\n","\n","R^2=1- Unexpected variation/Total variation\n","\n","\n","\n","R 2 =1− Total Variation Unexplained Variation​\n","\n","\n","Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model.​"],"metadata":{"id":"mq1c9me1I7Ik"}},{"cell_type":"markdown","source":[],"metadata":{"id":"vhN0QxucIwxo"}},{"cell_type":"markdown","source":["# **LINEAR REGRESSION**"],"metadata":{"id":"PBuqoHuzJkDv"}},{"cell_type":"markdown","source":["Regression models describe the relationship between variables by fitting a line to the observed data. Linear regression models use a straight line\n","\n","\n","Linear regression uses a linear approach to model the relationship between independent and dependent variables. In simple words its a best fit line drawn over the values of independent variables and dependent variable. In case of single variable, the formulais same as straight line equation having an intercept and slope.\n","\n","y_pred+B0+B1x\n","\n","\n","where\n","B0 AND B1\n","\n","are intercept and slope respectively.\n","\n","\n","In case of multiple features the formula translates into:\n","y_pred=B0+B1x1+B2x2+B3x3+........\n","\n","\n","where x_1,x_2,x_3 are the features values and\n","\n","B0,B1,B2......\n","\n","are weights assigned to each of the features. These become the parameters which the algorithm tries to learn using Gradient descent.\n","\n","\n","Gradient descent is the process by which the algorithm tries to update the parameters using a loss function . Loss function is nothing but the diffence between the actual values and predicted values(aka error or residuals). There are different types of loss function but this is the simplest one. Loss function summed over all observation gives the cost functions. The role of gradient descent is to update the parameters till the cost function is minimized i.e., a global minima is reached. It uses a hyperparameter 'alpha' that gives a weightage to the cost function and decides on how big the steps to take. Alpha is called as the learning rate. It is always necesarry to keep an optimal value of alpha as high and low values of alpha might make the gradient descent overshoot or get stuck at a local minima. There are also some basic assumptions that must be fulfilled before implementing this algorithm. They are:\n","\n","\n","No multicollinearity in the dataset.\n","\n","\n","Independent variables should show linear relationship with dv.\n","\n","Residual mean should be 0 or close to 0.\n","\n","There should be no heteroscedasticity i.e., variance should be constant along the line of best fit.\n","\n","Let us now implement our first model. We will be using LinearRegression from scikit library."],"metadata":{"id":"biXEh3RAJpuG"}},{"cell_type":"code","source":["# Define the model\n","model = LinearRegression()\n","\n","# Call the function to evaluate the model\n","test_metrics, train_metrics, y_pred_test, y_test, lambda__= evaluate_model(\"LinearRegression\", model, X, y)\n",""],"metadata":{"id":"jNmeXby7HaLY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_metrics"],"metadata":{"id":"J5bLQ-qmLMuD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","train_metrics\n"],"metadata":{"id":"H53O6AbYLRWP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Looks like our train set's r2 score value is 0.81 that means our model is able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**\n","\n","**The test set's r2_score is 0.82. This means our linear model is performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**"],"metadata":{"id":"9CG_A6IxMZcQ"}},{"cell_type":"markdown","source":["**Heteroscedasticity**\n","\n","Heteroscedasticity refers to a situation where the variance of the errors (residuals) is not constant across all levels of the independent variable(s) in a regression model.This violates one of the assumptions of linear regression, which is that the variance of the errors should be constant (homoscedastic) for all levels of the independent variable(s). If the plot shows a funnel shape, with the spread of residuals increasing or decreasing as the predicted values increase, this is an indication of heteroscedasticity."],"metadata":{"id":"iT2n20wvMkuF"}},{"cell_type":"code","source":["### Heteroscadacity - Residual plot\n","plt.scatter((y_pred_test),(y_test)-(y_pred_test))\n","plt.xlabel('Predicted Values')\n","plt.ylabel('Residuals')\n","plt.title('Residual Plot')\n","plt.show()"],"metadata":{"id":"yGVzzc7TMnFX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Actual Price vs predicte for Linear Regression plot\n","plt.figure(figsize=(10,6))\n","plt.plot(y_pred_test)\n","plt.plot(np.array(y_test))\n","plt.legend([\"Predicted\",\"Actual\"])\n","plt.xlabel('No of Test Data')\n","plt.show()"],"metadata":{"id":"4TCBtNOSMs8G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **RIDGE AND LASSO REGRESSION**\n","Ridge and Lasso Regression are types of Regularization techniques\n","\n","Regularization techniques are used to deal with overfitting and when the dataset is large\n","\n","Ridge and Lasso Regression involve adding penalties to the regression function"],"metadata":{"id":"GXWoL0iINEn3"}},{"cell_type":"markdown","source":["# **LASSO REGRESSION**\n","Lasso regression analysis is a shrinkage and variable selection method for linear regression models. The goal of lasso regression is to obtain the subset of predictors that minimizes prediction error for a quantitative response variable. It uses the Linear regression model with L1 regularization."],"metadata":{"id":"7BOR2xdFNYaO"}},{"cell_type":"code","source":["# Define the model\n","model = Lasso( alpha=1.0, max_iter=3000)\n","\n","# Call the function to evaluate the model\n","test_metrics, train_metrics, y_pred_test, y_test, lambda__= evaluate_model(\"lasso\", model, X, y)\n",""],"metadata":{"id":"5oA9jrlkNERy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Looks like train set's r2 score value is 0.37 that means our model is not able to capture most of the data variance.**\n","\n","**The test set's r2_score is 0.39. This means our linear model is not performing well on the data.**"],"metadata":{"id":"UNaNI8ZXNha-"}},{"cell_type":"code","source":["### Heteroscadacity- Residual plot\n","plt.scatter((y_pred_test),(y_test-y_pred_test))\n","plt.xlabel('Predicted Values')\n","plt.ylabel('Residuals')\n","plt.title('Residual Plot')\n","plt.show()"],"metadata":{"id":"8RCiDH2QM-wZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Plot the figure\n","plt.figure(figsize=(10,4))\n","plt.plot(np.array(y_pred_test))\n","plt.plot(np.array((y_test)))\n","plt.legend([\"Predicted\",\"Actual\"])\n","plt.show()"],"metadata":{"id":"XNtC-d6uLG8C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **RIDGE REGRESSION**\n","Ridge regression is a method of estimating the coefficients of regression models in scenarios where the independent variables are highly correlated. It uses the linear regression model with the L2 regularization method."],"metadata":{"id":"wkt3M0SuNwxB"}},{"cell_type":"code","source":["\n","#import the packages\n","from sklearn.linear_model import Ridge"],"metadata":{"id":"LuHUdPeYN00F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the model\n","model = Ridge()\n","\n","# Call the function to evaluate the model\n","test_metrics, train_metrics, y_pred_test, y_test, lambda__= evaluate_model(\"Ridge\", model, X, y)"],"metadata":{"id":"xqFptqB9ODQY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Looks like our train set's r2 score value is 0.81 that means our model is able to capture most of the data variance.**\n","\n","**The r2_score for the test set is 0.82. This means our linear model is performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**"],"metadata":{"id":"o7O1OFXdOG8o"}},{"cell_type":"code","source":["### Heteroscadacity - Residual plot\n","plt.scatter((y_pred_test),(y_test)-(y_pred_test))\n","plt.xlabel('Predicted Values')\n","plt.ylabel('Residuals')\n","plt.title('Residual Plot')\n","plt.show()\n",""],"metadata":{"id":"bs3AlEfAOLWY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Plot the figure\n","plt.figure(figsize=(10,8))\n","plt.plot((y_pred_test))\n","plt.plot((np.array(y_test)))\n","plt.legend([\"Predicted\",\"Actual\"])\n","plt.show()"],"metadata":{"id":"Z_lZu3-uOTUm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **ELASTIC NET REGRESSION**\n","Elastic Net regression is a linear regression model that combines both L1 (Lasso) and L2 (Ridge) regularization penalties to overcome some of the limitations of each individual method.\n","\n","The model introduces two hyperparameters, alpha and l1_ratio, which control the strength of the L1 and L2 penalties, respectively. Elastic Net regression is particularly useful when dealing with datasets that have high dimensionality and multicollinearity between features."],"metadata":{"id":"BnyAyJD4OX8O"}},{"cell_type":"code","source":["#import the packages\n","from sklearn.linear_model import ElasticNet\n","#a * L1 + b * L2\n","#alpha = a + b and l1_ratio = a / (a + b)"],"metadata":{"id":"0V45GZ-XOdjO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the model\n","model= ElasticNet(alpha=0.1, l1_ratio=0.5)\n","\n","# Call the function to evaluate the model\n","test_metrics, train_metrics, y_pred_test, y_test, lambda__= evaluate_model(\"ElasticNet\", model, X, y)"],"metadata":{"id":"i9WklMkwOiC6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","test_metrics"],"metadata":{"id":"_6LkHu3xOlwp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","train_metrics"],"metadata":{"id":"Cj78BagYOr_H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Looks like our train set's r2 score value is 0.64 that means our model is able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**\n","\n","**The r2_score for the test set is 0.66. This means our linear model is performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**"],"metadata":{"id":"Y3w5G919Ow1-"}},{"cell_type":"code","source":["### Heteroscadacity- Residual plo\n","plt.scatter((y_pred_test),(y_test)-(y_pred_test))\n","plt.xlabel('Predicted Values')\n","plt.ylabel('Residuals')\n","plt.title('Residual Plot')\n","plt.show()"],"metadata":{"id":"4M-nv6bLO2OO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Plot the figure\n","plt.figure(figsize=(10,4))\n","plt.plot(np.array(y_pred_test))\n","plt.plot((np.array(y_test)))\n","plt.legend([\"Predicted\",\"Actual\"])\n","plt.show()"],"metadata":{"id":"AAWaWZWDO6-A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **DECISION TREE**\n","A decision tree is a type of supervised machine learning algorithm that is commonly used for classification and regression tasks. It works by recursively splitting the data into subsets based on the values of certain attributes, ultimately arriving at a set of decision rules that can be used to classify or predict outcomes for new data.\n","\n","first i tried with these parameteres\n","\n","criterion='friedman_mse', max_depth=8, max_features=9, max_leaf_nodes=100,\n","\n","but the default one is giving the better result\n","\n","letter we will do hyper parameter tunning"],"metadata":{"id":"w4wlR-CxPDPA"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeRegressor\n","\n","decision_regressor = DecisionTreeRegressor()"],"metadata":{"id":"RkSjoKcQPLQ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the model\n","model= decision_regressor\n","\n","# Call the function to evaluate the model\n","test_metrics, train_metrics, y_pred_test, y_test, lambda__= evaluate_model(\"decision_regressor\", model, X, y)\n",""],"metadata":{"id":"xO7fgsiAPOdU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**It seems that our decision tree model with default parameters has achieved perfect performance on the training set. The R-squared value of 1.0 indicates that the model is capable of capturing all the variance in the data**\n","\n","**The r2_score for the test set is 0.84. This means our linear model is performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**"],"metadata":{"id":"XsttwOK3PSof"}},{"cell_type":"code","source":["### Heteroscadacity - Residual plot\n","plt.scatter((y_pred_test),(y_test)-(y_pred_test))\n","plt.xlabel('Predicted Values')\n","plt.ylabel('Residuals')\n","plt.title('Residual Plot')\n","plt.show()\n",""],"metadata":{"id":"IqRp5CRsPfLI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Plot the figure\n","plt.figure(figsize=(10,8))\n","plt.plot((np.array(y_pred_test)))\n","plt.plot(np.array((y_test)))\n","plt.legend([\"Predicted\",\"Actual\"])\n","plt.show()"],"metadata":{"id":"Ly2bYan-Pj7g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **RANDOM FOREST**"],"metadata":{"id":"25cdWQC1PolN"}},{"cell_type":"code","source":["#import the packages\n","\n","from sklearn.ensemble import RandomForestRegressor\n","\n","# Create an instance of the RandomForestRegressor\n","rf_model = RandomForestRegressor()"],"metadata":{"id":"ciif4GdMPtMf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rf_model.fit(X_train_transformed,y_train)"],"metadata":{"id":"q5IsT2AcPxAl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.**\n","**On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. **"],"metadata":{"id":"rlyZMNN2P6Z1"}},{"cell_type":"code","source":["# Define the model\n","rf_model = RandomForestRegressor()\n","\n","# Call the function to evaluate the model\n","test_metrics, train_metrics, y_pred_test, y_test, lambda__= evaluate_model(\"RandomForestRegressor\", rf_model, X, y)\n",""],"metadata":{"id":"mfPc6z0VQFmw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Looks like our train set's r2 score value is 0.99 that means our model is able to capture most of the data variance..**\n","\n","**The r2_score for the test set is 0.93. This means our linear model is performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**"],"metadata":{"id":"RYljHFcnQLIo"}},{"cell_type":"code","source":["### Heteroscadacity- Residual plot\n","plt.scatter((y_pred_test),(y_test)-(y_pred_test))\n","plt.xlabel('Predicted Values')\n","plt.ylabel('Residuals')\n","plt.title('Residual Plot')\n","plt.show()"],"metadata":{"id":"fpPjeFg3QRSm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rf_model.feature_importances_"],"metadata":{"id":"16kuAZZiQVjS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Features Stored**"],"metadata":{"id":"EbZBhlSlQYmH"}},{"cell_type":"code","source":["importances = rf_model.feature_importances_\n","\n","importance_dict = {'Feature' : list(X_train_transformed_df.columns),\n","                   'Feature Importance' : importances}\n","\n","importance_df = pd.DataFrame(importance_dict)"],"metadata":{"id":"h_QU1l-cQclA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"],"metadata":{"id":"i1ufOf2pQiOI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","importance_df.sort_values(by=['Feature Importance'],ascending=False)"],"metadata":{"id":"Sl5PNHvYQmUI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features = X_train_transformed_df.columns\n","importances = rf_model.feature_importances_\n","indices = np.argsort(importances)"],"metadata":{"id":"ksWqbkcVQriQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Plot the figure\n","plt.figure(figsize=(10,20))\n","plt.title('Feature Importance')\n","plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n","plt.yticks(range(len(indices)), [features[i] for i in indices])\n","plt.xlabel('Relative Importance')\n","\n","plt.show()"],"metadata":{"id":"6QWCuW0dQuQW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **XGBOOST**"],"metadata":{"id":"Y3Q0tRLBQyQH"}},{"cell_type":"code","source":["#import the packages\n","import xgboost as xgb\n","\n","# Create an instance of the XGBRegressor\n","model = xgb.XGBRegressor()\n","\n","# Call the function to evaluate the model\n","test_metrics, train_metrics, y_pred_test, y_test, lambda__= evaluate_model(\"GradientBoostingRegressor\", model, X, y)\n",""],"metadata":{"id":"2HlTFqXLQ2Pg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Looks like our train set's r2 score value is 0.98 that means our model is able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"],"metadata":{"id":"N4JumC2zRGtF"}},{"cell_type":"markdown","source":["**The r2_score for the test set is 0.94. This means our is performing well on the data. **"],"metadata":{"id":"KdF2f99fROMl"}},{"cell_type":"code","source":["### Heteroscadacity\n","plt.scatter((y_pred_test),(y_test)-(y_pred_test))\n","plt.xlabel('Predicted Values')\n","plt.ylabel('Residuals')\n","plt.title('Residual Plot')\n","plt.show()"],"metadata":{"id":"wDpU-O9MRcp3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.fit(X_train_transformed,y_train)"],"metadata":{"id":"cAmcSGDuRiCo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.**\n","**On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.**"],"metadata":{"id":"tcPg-IabRqu_"}},{"cell_type":"code","source":["model.feature_importances_"],"metadata":{"id":"1UuwdyqcRyTe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Features Stored**"],"metadata":{"id":"_UQXfIjsR2jA"}},{"cell_type":"code","source":["model.fit(X_train_transformed,y_train)"],"metadata":{"id":"QfpBFH_YR7X1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.**\n","**On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.**"],"metadata":{"id":"YbjOmuEcSFQR"}},{"cell_type":"code","source":["importances = model.feature_importances_\n","\n","importance_dict = {'Feature' : list(X_train_transformed_df.columns),\n","                   'Feature Importance' : importances}\n","\n","importance_df = pd.DataFrame(importance_dict)"],"metadata":{"id":"Vx3okaRaSKUu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)\n",""],"metadata":{"id":"S9iRASZ8SNim"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["importance_df.sort_values(by=['Feature Importance'],ascending=False)"],"metadata":{"id":"SlMdbKRhSRGv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features = X_train_transformed_df.columns\n","importances = model.feature_importances_\n","indices = np.argsort(importances)"],"metadata":{"id":"3lxampiySVDh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Plot the figure\n","plt.figure(figsize=(10,20))\n","plt.title('Feature Importance')\n","plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n","plt.yticks(range(len(indices)), [features[i] for i in indices])\n","plt.xlabel('Relative Importance')\n","\n","plt.show()"],"metadata":{"id":"4M4zDxmsSaOh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **GRADIENT BOOSTING**"],"metadata":{"id":"65AuOw2DSfB1"}},{"cell_type":"code","source":["#import the packages\n","from sklearn.ensemble import GradientBoostingRegressor\n","# Create an instance of the GradientBoostingRegressor\n","gb_model = GradientBoostingRegressor()\n"],"metadata":{"id":"yDlf3ir4Si9X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the model\n","model= GradientBoostingRegressor()\n","\n","# Call the function to evaluate the model\n","test_metrics, train_metrics, y_pred_test, y_test, lambda__= evaluate_model(\"GradientBoostingRegressor\", model, X, y)\n",""],"metadata":{"id":"BVGLveQQSmVh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Looks like our train set's r2 score value is 0.89 that means our model is able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"],"metadata":{"id":"JRwVN8PRSrij"}},{"cell_type":"markdown","source":["**The r2_score for the test set is 0.88. This means our model is performing well on the data."],"metadata":{"id":"kqVUfU7lSvPB"}},{"cell_type":"code","source":["### Heteroscadacity\n","plt.scatter((y_pred_test),(y_test)-(y_pred_test))\n","plt.xlabel('Predicted Values')\n","plt.ylabel('Residuals')\n","plt.title('Residual Plot')\n","plt.show()"],"metadata":{"id":"hgwUBNjKRpn9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Features Stored**"],"metadata":{"id":"396w3MD-S2We"}},{"cell_type":"code","source":["gb_model = GradientBoostingRegressor()"],"metadata":{"id":"qEI_A3UvS9tV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gb_model.fit(X_train_transformed,y_train)"],"metadata":{"id":"CkEF0QV3TCZ0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.**\n","**On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.**"],"metadata":{"id":"20P9VUzDS9F_"}},{"cell_type":"code","source":["importances = gb_model.feature_importances_\n","\n","importance_dict = {'Feature' : list(X_train_transformed_df.columns),\n","                   'Feature Importance' : importances}\n","\n","importance_df = pd.DataFrame(importance_dict)"],"metadata":{"id":"FtLx06imTRlm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"],"metadata":{"id":"E--Qz5ZYTVKF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["importance_df.sort_values(by=['Feature Importance'],ascending=False)"],"metadata":{"id":"QVH1DqIfTYkh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features = X_train_transformed_df.columns\n","importances = gb_model.feature_importances_\n","indices = np.argsort(importances)"],"metadata":{"id":"I7b-XjXjTeBY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Plot the figure\n","plt.figure(figsize=(10,20))\n","plt.title('Feature Importance')\n","plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n","plt.yticks(range(len(indices)), [features[i] for i in indices])\n","plt.xlabel('Relative Importance')\n","\n","plt.show()"],"metadata":{"id":"WaBAKwNeThql"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Hyperparameter tuning**"],"metadata":{"id":"bLE5Hd3sTnIw"}},{"cell_type":"markdown","source":["**Note :**\n","\n","Hyperparameter tuning has been previously conducted for this model\n","\n","Uncommenting the code below will rerun hyperparameter tuning, which may take significant time\n","\n","For efficiency, the tuned parameters have been saved for direct use\n","Before proceding to try next models, let us try to tune some hyperparameters and see if the performance of our model improves.\n","\n","\n","Hyperparameter tuning is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a model argument whose value is set before the learning process begins. The key to machine learning algorithms is hyperparameter tuning."],"metadata":{"id":"BeK5MiSkTtx8"}},{"cell_type":"markdown","source":["# **GridSearchCV ,RandomSearchCV and BayesSearchCV**"],"metadata":{"id":"3Yvr19U3T7ZR"}},{"cell_type":"markdown","source":["BayesSearchCV is a hyperparameter optimization technique provided by the scikit-optimize library, which is an efficient way to tune hyperparameters of machine learning models. Here are some notes on BayesSearchCV:\n","\n","Bayesian Optimization: BayesSearchCV employs Bayesian optimization, which is a probabilistic model-based optimization approach. It builds a probabilistic model of the objective function (such as cross-validation score) and uses it to select the most promising hyperparameters for evaluation.\n","\n","Efficiency: Bayesian optimization is particularly useful when the objective function is expensive to evaluate, such as when training machine learning models with large datasets or complex models. It efficiently searches the hyperparameter space by selecting the next set of hyperparameters based on the posterior distribution of the objective function.\n","\n","Search Space: It allows defining a search space for hyperparameters using different types of distributions such as Real (continuous), Integer (discrete), and Categorical (categorical).\n","\n","Scalability: BayesSearchCV is scalable and can handle a large number of hyperparameters and a large search space. It efficiently explores the hyperparameter space and converges to optimal or near-optimal hyperparameters relatively quickly.\n","\n","Cross-validation: It performs cross-validation during the search for hyperparameters to estimate the performance of each set of hyperparameters. This ensures that the selected hyperparameters generalize well to unseen data.\n","\n","Parallelization: BayesSearchCV supports parallelization, allowing it to explore multiple sets of hyperparameters simultaneously. This can significantly reduce the overall optimization time, especially when running on a multi-core CPU or distributed computing environment.\n","\n","Integration with scikit-learn: BayesSearchCV is compatible with scikit-learn's API, making it easy to integrate into existing machine learning pipelines. It can be used as a drop-in replacement for other hyperparameter search techniques provided by scikit-learn, such as GridSearchCV and RandomizedSearchCV.\n","\n","Customization: It provides options for customizing the optimization process, such as the number of iterations (n_iter), the cross-validation strategy (cv), verbosity level (verbose), and the number of parallel jobs (n_jobs).\n","\n","Best Parameters: Once the optimization is complete, BayesSearchCV returns the best set of hyperparameters found during the search, which can then be used to train the final model.\n","\n","Overall, BayesSearchCV is a powerful and efficient technique for hyperparameter tuning, especially in scenarios where traditional grid search or random search may be inefficient or impractical. It combines the advantages of probabilistic modeling, parallelization, and scikit-learn integration to provide an effective solution for optimizing machine learning models."],"metadata":{"id":"IF8GhL_SUBVW"}},{"cell_type":"markdown","source":["# **Gradient Boost Regressor with GridSearchCV**"],"metadata":{"id":"5Km4J_S1UFyd"}},{"cell_type":"code","source":["# Number of trees\n","n_estimators = [50,80,100]\n","\n","# Maximum depth of trees\n","max_depth = [4,6,8]\n","\n","# Minimum number of samples required to split a node\n","min_samples_split = [50,100,150]\n","\n","# Minimum number of samples required at each leaf node\n","min_samples_leaf = [40,50]\n","\n","# HYperparameter Grid\n","param_dict = {'n_estimators' : n_estimators,\n","              'max_depth' : max_depth,\n","              'min_samples_split' : min_samples_split,\n","              'min_samples_leaf' : min_samples_leaf}"],"metadata":{"id":"USd7Bx3tULJn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["param_dict"],"metadata":{"id":"_9EnKOtQUPBF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from sklearn.model_selection import GridSearchCV\n","# from sklearn.ensemble import GradientBoostingRegressor\n","\n","# # Create an instance of the GradientBoostingRegressor\n","# gb_model = GradientBoostingRegressor()\n","\n","# # Grid search\n","# param_dict = {'learning_rate': [0.1, 0.01],\n","#               'n_estimators': [50, 100],\n","#               'max_depth': [3, 5]}\n","\n","# gb_grid = GridSearchCV(estimator=gb_model,\n","#                        param_grid=param_dict,\n","#                        cv=5, verbose=2, n_jobs=-1)\n","\n","# gb_grid.fit(X_train_transformed, y_train)"],"metadata":{"id":"aRX1AvM8UUV-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# gb_grid.best_estimator_"],"metadata":{"id":"hbCm8sG2UXon"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# gb_optimal_model = gb_grid.best_estimator_"],"metadata":{"id":"RviCAQINUaVy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# gb_grid.best_params_"],"metadata":{"id":"FtdmXthzUdCr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = GradientBoostingRegressor(learning_rate=0.1, max_depth=5, n_estimators=100)\n","\n","# Call the function to evaluate the model\n","test_metrics, train_metrics, y_pred_test, y_test, lambda__= evaluate_model(\"GradientBoostingRegressor_GridSearchCV\", model, X, y)\n",""],"metadata":{"id":"-KpBjieJUgHn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Gradient BoostRegressor with BayesSearchCV**"],"metadata":{"id":"klOTU1x7Uk0N"}},{"cell_type":"code","source":["# # Create an instance of the GradientBoostingRegressor\n","# gb_model = GradientBoostingRegressor()\n","\n","# # Parameter grid for grid search\n","# param_dict = {\n","#     'learning_rate': [0.1, 0.01, 0.2],  # Fixed typo here\n","#     'n_estimators': [50, 100, 110],     # Corrected the range here\n","#     'max_depth': [4, 5, 6],\n","#     'min_samples_split': Integer(2, 10),  # Add min_samples_split as an integer range\n","#     'min_samples_leaf': Integer(1, 5),     # Add min_samples_leaf as an integer range\n","#     'subsample': Real(0.5, 1.0, prior='uniform'),  # Add subsample as a real-valued range\n","#     'max_features': ['auto', 'sqrt', 'log2']\n","# }\n","\n","# # Bayesian search\n","# gb_bayesian = BayesSearchCV(estimator=gb_model,\n","#                              search_spaces=param_dict,\n","#                              n_iter=50, cv=5, verbose=2, n_jobs=-1)\n","\n","# gb_bayesian.fit(X_train_transformed, y_train)\n","\n","# # Get the best parameters\n","# best_params = gb_bayesian.best_params_\n","# print(\"Best parameters for GradientBoostingRegressor:\", best_params)"],"metadata":{"id":"brjtpkFmUr4j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define hyperparameters\n","params = {\n","    'learning_rate': 0.2,\n","    'max_depth': 6,\n","    'max_features': 'auto',\n","    'min_samples_leaf': 5,\n","    'min_samples_split': 10,\n","    'n_estimators': 110,\n","    'subsample': 1.0\n","}\n","\n","# Define the GradientBoostingRegressor model with specified hyperparameters\n","model_gb_B_search_cv = GradientBoostingRegressor(**params)\n","test_metrics, train_metrics, y_pred_test, y_test, lambda__= evaluate_model(\"GradientBoostingRegressor_BayesSearchCV\", model_gb_B_search_cv, X, y)\n",""],"metadata":{"id":"L7RAP5oeUxhu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_gb_B_search_cv.fit(X_train_transformed,y_train)"],"metadata":{"id":"Qdby9Lt3U4Qg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.**\n","**On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org**."],"metadata":{"id":"cnkwEMHjVAbw"}},{"cell_type":"code","source":["model_gb_B_search_cv"],"metadata":{"id":"rl3BexEEVFBJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.**\n","**On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.**"],"metadata":{"id":"3R7Blhy3VJcP"}},{"cell_type":"code","source":["\n","### Heteroscadacity\n","plt.scatter((y_pred_test),(y_test)-(y_pred_test))\n","plt.xlabel('Predicted Values')\n","plt.ylabel('Residuals')\n","plt.title('Residual Plot')\n","plt.show()"],"metadata":{"id":"F6nDYWgcVRJF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_gb_B_search_cv.feature_importances_"],"metadata":{"id":"aMPcIYNJVV-s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["importances = model_gb_B_search_cv.feature_importances_\n","\n","importance_dict = {'Feature' : list(X_train_transformed_df.columns),\n","                   'Feature Importance' : importances}\n","\n","importance_df = pd.DataFrame(importance_dict)"],"metadata":{"id":"NBFBCP7-Vf-2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"],"metadata":{"id":"pP721-qRVhj9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_importance_df_list = []\n","tuned_GBR_feature_importance = importance_df\n","feature_importance_df_list.append(tuned_GBR_feature_importance)"],"metadata":{"id":"l3QR2KYlVnSl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["importance_df.sort_values(by=['Feature Importance'],ascending=False)"],"metadata":{"id":"giuoX3gzVuK9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features = X_train_transformed_df.columns\n","importances = model_gb_B_search_cv.feature_importances_\n","indices = np.argsort(importances)"],"metadata":{"id":"9UtgHG8kVzme"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Plot the figure\n","plt.figure(figsize=(10,20))\n","plt.title('Feature Importance')\n","plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n","plt.yticks(range(len(indices)), [features[i] for i in indices])\n","plt.xlabel('Relative Importance')\n","\n","plt.show()"],"metadata":{"id":"kd_HEjieV2vH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **RandomForest with RandomSearchCV**"],"metadata":{"id":"cn0fKVD9V9bc"}},{"cell_type":"code","source":["# from sklearn.model_selection import RandomizedSearchCV\n","# from sklearn.ensemble import RandomForestRegressor\n","\n","# # Create an instance of the RandomForestRegressor\n","# rf_model = RandomForestRegressor()\n","\n","# # Define the hyperparameter grid\n","# param_dict = {\n","#     'n_estimators': [100, 200, 500],\n","#     'max_depth': [None, 10, 20],\n","#     'min_samples_split': [2, 5, 10],\n","#     'min_samples_leaf': [1, 2, 4],\n","#     'max_features': ['auto', 'sqrt', 'log2']\n","# }\n","\n","# # Randomized search\n","# rf_random = RandomizedSearchCV(estimator=rf_model,\n","#                                param_distributions=param_dict,\n","#                                n_iter=100, cv=5, verbose=2,\n","#                                random_state=42, n_jobs=-1)\n","\n","# # Fit the randomized search\n","# rf_random.fit(X_train_transformed, y_train)\n","\n","# # Get the best parameters\n","# best_params = rf_random.best_params_\n","# print(\"Best parameters for RandomForestRegressor:\", best_params)"],"metadata":{"id":"dnWCrAJHWC03"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","params = {\n","    'n_estimators': 500,\n","    'min_samples_split': 5,\n","    'min_samples_leaf': 1,\n","    'max_features': 'auto',\n","    'max_depth': None\n","}\n","\n","# Create the RandomForestRegressor instance with the specified parameters\n","RandomForestRegressor_RandomSearch = RandomForestRegressor(**params)\n","# Call the function to evaluate the model\n","test_metrics, train_metrics, y_pred_test, y_test, lambda__= evaluate_model(\"RandomForestRegressor_RandomSearch\", RandomForestRegressor_RandomSearch, X, y)\n",""],"metadata":{"id":"vsFmy96vWoTx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Features Stored TunedRandomForest**"],"metadata":{"id":"kHRXz_7PWzyI"}},{"cell_type":"code","source":["importances = RandomForestRegressor_RandomSearch.feature_importances_\n","\n","importance_dict = {'Feature' : list(X_train_transformed_df.columns),\n","                   'Feature Importance' : importances}\n","\n","importance_df = pd.DataFrame(importance_dict)\n",""],"metadata":{"id":"ep1maPxxW3RW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"],"metadata":{"id":"2kK2GOkoW62A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tuned_RF_feature_importance = importance_df\n","feature_importance_df_list.append(tuned_RF_feature_importance)\n",""],"metadata":{"id":"vOezHVjLW-A2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["importance_df.sort_values(by=['Feature Importance'],ascending=False)"],"metadata":{"id":"MpxFcMRbXBPV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features = X_train_transformed_df.columns\n","importances = RandomForestRegressor_RandomSearch.feature_importances_\n","indices = np.argsort(importances)"],"metadata":{"id":"UoBbuA4rXGCA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Plot the figure\n","plt.figure(figsize=(10,20))\n","plt.title('Feature Importance')\n","plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n","plt.yticks(range(len(indices)), [features[i] for i in indices])\n","plt.xlabel('Relative Importance')\n","\n","plt.show()"],"metadata":{"id":"NKVlfJwEXJpa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **XGBoostRegressor with BayesSearchCV**"],"metadata":{"id":"Hrl2MQ51XPYO"}},{"cell_type":"code","source":["# from skopt import BayesSearchCV\n","# import xgboost as xgb\n","\n","# # Define the parameter search space\n","# param_space = {\n","#     'n_estimators': (100, 2000),\n","#     'max_depth': (3, 10),\n","#     'learning_rate': (0.01, 0.1, 'log-uniform'),\n","#     'subsample': (0.6, 0.8),\n","#     'colsample_bytree': (0.8, 1.0),\n","#     'min_child_weight': (1, 10),\n","#     'gamma': (0, 5),  # Add gamma\n","#     'reg_alpha': (0, 1),\n","#     'reg_lambda': (0, 1)\n","# }\n","\n","# # Define the XGBRegressor model\n","# xgb_model = xgb.XGBRegressor()\n","\n","# # Perform Bayesian optimization\n","# bayes_search = BayesSearchCV(estimator=xgb_model,\n","#                              search_spaces=param_space,\n","#                              n_iter=50,  # adjust as needed\n","#                              cv=5,  # cross-validation folds\n","#                              random_state=0,\n","#                              verbose=2,\n","#                              n_jobs=-1)\n","\n","# # Fit the Bayesian search\n","# bayes_search.fit(X_train_transformed, y_train)\n","\n","# # Get the best model and best parameters\n","# best_model = bayes_search.best_estimator_\n","# best_params = bayes_search.best_params_\n","\n","# # Print the best parameters\n","# print(\"Best parameters:\", best_params)"],"metadata":{"id":"ahUzT2bfXU9Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import xgboost as xgb\n","\n","# Define hyperparameters\n","params = {\n","    'colsample_bytree': 1.0,\n","    'gamma': 0,\n","    'learning_rate': 0.022495259692763524,\n","    'max_depth': 8,\n","    'min_child_weight': 1,\n","    'n_estimators': 1691,\n","    'reg_alpha': 0,\n","    'reg_lambda': 1,\n","    'subsample': 0.6\n","}\n","\n","# Define the XGBRegressor model with specified hyperparameters\n","model_xgb_B_search_cv = xgb.XGBRegressor(**params)\n","\n","# Call the function to evaluate the model\n","test_metrics, train_metrics, y_pred_test, y_test, lambda__= evaluate_model(\"XGBRegressor_BayesSearchCV\", model_xgb_B_search_cv, X, y)\n",""],"metadata":{"id":"SlrWtnZHXXDO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Looks like our train set's r2 score value is 0.998 that means our model is able to capture most of the data variance..**"],"metadata":{"id":"C28B1slOXiW4"}},{"cell_type":"markdown","source":["Hyperparameter tunning certainly showed a better result, r2 was 0.947 on test and mae and rmse was lowered. Overall model show good result."],"metadata":{"id":"Ai5KQqnqXkkR"}},{"cell_type":"markdown","source":["# **Features Stored TunedXGBoostRegressor**"],"metadata":{"id":"C5A2ZX0NXx85"}},{"cell_type":"code","source":["importances = model_xgb_B_search_cv.feature_importances_\n","\n","importance_dict = {'Feature' : list(X_train_transformed_df.columns),\n","                   'Feature Importance' : importances}\n","\n","importance_df = pd.DataFrame(importance_dict)\n",""],"metadata":{"id":"QsnYcG95XxUI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"],"metadata":{"id":"GO5XlIo7X5wP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tuned_XGBR_feature_importance = importance_df\n","feature_importance_df_list.append(tuned_XGBR_feature_importance)\n",""],"metadata":{"id":"FSQHH5lkX8s2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["importance_df.sort_values(by=['Feature Importance'],ascending=False)"],"metadata":{"id":"cMqWd8JuX_hd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features = X_train_transformed_df.columns\n","importances = model_xgb_B_search_cv.feature_importances_\n","indices = np.argsort(importances)"],"metadata":{"id":"pbH0wq3yYEB4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Plot the figure\n","plt.figure(figsize=(10,20))\n","plt.title('Feature Importance')\n","plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n","plt.yticks(range(len(indices)), [features[i] for i in indices])\n","plt.xlabel('Relative Importance')\n","\n","plt.show()"],"metadata":{"id":"RMGzUczRYG32"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Pickle file**"],"metadata":{"id":"C0_H2Z2lYL7h"}},{"cell_type":"markdown","source":["# 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"],"metadata":{"id":"p1ppbbLOYRcv"}},{"cell_type":"code","source":["# Import pickle\n","import pickle\n","\n","# Save the best model (XGB)\n","pickle.dump(model_xgb_B_search_cv, open('xgbmodel.pkl', 'wb'))"],"metadata":{"id":"ecNy8fVZYVI0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Again Load the saved model file and try to predict unseen data for a sanity check."],"metadata":{"id":"4kcdIJ8aYawP"}},{"cell_type":"code","source":["# Load the File and predict unseen data.\n","pickled_model = pickle.load(open('xgbmodel.pkl', 'rb'))\n","\n","instance = X_test_transformed[50, :].reshape(1, -1)\n","\n","# Testing on one instance\n","pickled_model.predict(instance)"],"metadata":{"id":"QFY851ktYfai"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Model is successfully created and ready for deployment on a live server for a real user interaction.**"],"metadata":{"id":"_JQ3Xd2uYkd7"}},{"cell_type":"markdown","source":[],"metadata":{"id":"MVxaV0EqVfkN"}},{"cell_type":"markdown","source":["# **Conclusion**"],"metadata":{"id":"gCX9965dhzqZ"}},{"cell_type":"code","source":[],"metadata":{"id":"LxX4tMRS9kzx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Results**"],"metadata":{"id":"Pc7XHFQD4RbJ"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Add a new column 'Set' to indicate whether it's from the training or testing set\n","for df_train in models_train_res_list:\n","    df_train['Set'] = 'Train'\n","\n","for df_test in models_test_res_list:\n","    df_test['Set'] = 'Test'\n","\n","# Concatenate DataFrames vertically within each list\n","train_df = pd.concat(models_train_res_list, ignore_index=True)\n","test_df = pd.concat(models_test_res_list, ignore_index=True)"],"metadata":{"id":"wWEjS-5a4aQb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df.sort_values(by = 'R-squared',ascending = False)"],"metadata":{"id":"oFeuV29R4eEF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df.sort_values(by = 'R-squared',ascending = False)\n"],"metadata":{"id":"VORzG9mx4k_g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Based on the sorted R-squared values:"],"metadata":{"id":"Cx3amfuH4zwk"}},{"cell_type":"markdown","source":["## **For the Test Set:**\n","\n","1.XGBRegressor_BayesSearchCV has the highest R-squared value of 0.947145, indicating a strong fit of the model to the test data.\n","2.Following closely is GradientBoostingRegressor with a slightly lower R-squared value of 0.938084.\n","3.The worst-performing models on the test set are ElasticNet and lasso with R-squared values of 0.657462 and 0.393221, respectively.\n","\n","**For the Training Set:**\n","\n","1.decision_regressor has a perfect R-squared value of 1.000000, which might indicate overfitting as it performed exceptionally well on the training data.\n","2.XGBRegressor_BayesSearchCV follows with a very high R-squared value of 0.998292, suggesting strong performance on the training data.\n","3.ElasticNet and lasso have the lowest R-squared values on the training set, indicating poor performance compared to other models.\n","\n","**Conclusion:**\n","\n","1-XGBRegressor_BayesSearchCV appears to be the most promising model as it performs well on both the test and training sets, with high R-squared values indicating good fit to the data.\n","\n","2-Models like GradientBoostingRegressor and RandomForestRegressor also show promising performance, although they might require further optimization or regularization to avoid overfitting.\n","\n","3-ElasticNet and lasso seem to struggle with both the test and training data, indicating that they might not be suitable for this particular dataset without further tuning or feature engineering.\n","In summary, XGBRegressor_BayesSearchCV seems to be the best-performing model based on the provided R-squared values, exhibiting strong performance on both the training and test sets."],"metadata":{"id":"BOanbnh944Dv"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Assuming feature_importance_df_list is a list containing your DataFrames\n","\n","# Example:\n","# feature_importance_df_list = [df1, df2, df3, ...]\n","\n","# Initialize merged_df with the first DataFrame in the list\n","merged_df = feature_importance_df_list[0]\n","\n","# Merge all DataFrames in feature_importance_df_list on the 'Feature' column\n","for df in feature_importance_df_list[1:]:\n","    merged_df = pd.merge(merged_df, df, on='Feature', how='outer')\n","\n","# Resulting merged DataFrame will contain all features from all DataFrames"],"metadata":{"id":"5c3zPmV_5xE4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Renaming the columns\n","Feature_imprtance_df = merged_df.rename(columns={\n","    'Feature Importance_x': 'tuned_GBR_feature_importance',\n","    'Feature Importance_y': 'tuned_XGBR_feature_importance',\n","    'Feature Importance': 'tuned_RF_feature_importance'\n","})"],"metadata":{"id":"ovFy2lIZ53qp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate mean across each row for all numeric columns except the first one\n","mean_values = Feature_imprtance_df.iloc[:, 1:].mean(axis=1)\n","\n","# Create a new DataFrame with the first column and the mean values\n","avg_feature_importance = pd.DataFrame({\n","    'Feature': Feature_imprtance_df['Feature'],  # Keep the first column\n","    'Mean': mean_values  # Add a new column with mean values\n","})"],"metadata":{"id":"LMF5vtaT573o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["avg_feature_importance.sort_values(by='Mean',ascending = False).head()"],"metadata":{"id":"LY2XcU225-47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sum across the specified columns and assign to a new column 'all_zero'\n","Feature_imprtance_df['all_zero'] = Feature_imprtance_df[['tuned_GBR_feature_importance', 'tuned_XGBR_feature_importance', 'tuned_RF_feature_importance']].sum(axis=1)\n","\n",""],"metadata":{"id":"_5vKciUa6Cnu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Feature_imprtance_df[Feature_imprtance_df['all_zero']!=0]"],"metadata":{"id":"xddC4Qo66Gzn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Conclusion**\n","\n","We train a model to predict the number of rented bike count in given weather conditions. First, we do Exploratory Data Analysis on the data set. We look for null values that is not found in dataset and outliers and appropriately modify them. We also perform correlation analysis to extract out the important and relevant feature set and later perform feature engineering.\n","\n","\n","# **Here are key insights and conclusion:**\n","**1-Insights from Analysis:**\n","\n","Peak Hours: Bike demand shows peek around 8-9 AM in the morning and 6 - 7pm in the evening.\n","\n","Monthly Trends: June recorded the highest demand, likely due to favorable weather conditions.\n","\n","Feature Importance: Temperatue, Functioning_Day_Yes, Humidity, Rainfall and Solar radiation are major driving factors for the Bike rent demand.\n","Weather Impact: Bike demand is more on clear days than on snowy or rainy days.\n","\n","Seasonal Preference: Rental activity peaked during summer months, contrasting with reduced uptake in winter.\n","\n","Temperature Range: Temperature range from 22 to 25(°C) has more demand for bike.\n","\n","**Model Performance:**\n","\n","The XG Boost Regressor with BayesSearchCV exhibited outstanding performance, boasting an R-squared score of 0.95, positioning it as a reliable predictor of bike demand.\n","\n","Feature importance analysis underscored the significance of temperature, functioning day, humidity, rainfall, and solar radiation in forecasting demand.\n","\n","**Exploratory Data Analysis (EDA):**\n","\n","Rigorous EDA involved scrutinizing feature distributions, transforming dependent variables, and adeptly handling categorical data.\n","Correlation analysis identified critical features, with categorical variables appropriately one-hot encoded.\n","\n","**Future Considerations:**\n","\n","Model Deployment: Consider deploying the trained model for real-time predictions to optimize bike rental operations efficiently.\n","\n","Continuous Monitoring: Given the dataset's temporal nature and sensitivity to weather fluctuations, ongoing model recalibration and monitoring are imperative.\n","\n","Further Research: Explore advanced time-series techniques and incorporate additional external factors like events or holidays to refine predictions further.\n","\n","Scalability: Extend findings to similar bike-sharing systems in other cities to enhance operational efficiency and stakeholder outcomes.\n","\n","Although the current analysis may be insightful, it is important to note that the dataset is time-dependent and variables such as temperature, windspeed and solar radiation may not always remain consistent. As a result there may be situations where the model fails to perform well. As field of machine learning is constantly evolving, it is necessary to stay up-to-date with the latest developments and be prepared to handle unexpected scenarios. Maintaining a strong understanding of Machine Learning concepts will undoubtely provide an advantage in staying ahead in the future.Write the conclusion here."],"metadata":{"id":"Fjb1IsQkh3yE"}},{"cell_type":"markdown","source":["### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"],"metadata":{"id":"gIfDvo9L0UH2"}}]}